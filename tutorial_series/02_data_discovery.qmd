---
title: "2. Data Discovery"
author: "Cob Staines"
date: today
toc: true
format:
  html: 
    code-line-numbers: false
  pdf:
    code-line-numbers: false
    geometry: 
      - top=30mm
      - left=30mm
---

```{r setup, include=FALSE}
librarian::shelf(reticulate)
# use_python("/home/cob/miniconda3/envs/ribbitr/bin/python")
use_condaenv("ribbitr")
```

*This tutorial is available as a [.qmd on Github](https://github.com/RIBBiTR-BII/ribbitr-data-access/tree/main/tutorial_series).*

# Motivation
- Explore what data are currently available on the database
- Identify structure of data of interest to inform access

::: {.panel-tabset}

# R
Let's set up our environment to get ready to explore the database.

## Load packages

```{r, message=FALSE}
# minimal packages for RIBBiTR DB data discovery
librarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)
```

## Establish database connection

```{r}
# establish database connection
dbcon = hopToDB("ribbitr")
```

## Load database metadata
### Data structure: Schemas, tables, columns and rows
The RIBBiTR database is organized into "schemas" (think of these as folders), which can contain any number of tables. Each table consists of columns ("variables") and rows ("entries").

### Metadata: Data about data
We keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let's learn what tables are present in the data by loading the table metadata.

### Table Metadata

```{r}
# load table "all_tables" from schema "public"
mdt = tbl(dbcon, Id("public", "all_tables")) %>%
  collect()
```

#### Some basic database commands

Before we take a look at the metadata you just pulled, let's understand the command we just ran.

- `dplyr::tbl()` - This function is used to create a "lazy" table from a data source. To specify the source, we provide the database connection `dbcon`, as well as a pointer or "address" for the table of interest using the `Id()` function. A "lazy" table means that the data only pulled when explicitly asked for. See `collect()` below.
- `dbplyr::Id()` - This function is a pointer to pass hierarchical table identifiers (you can think of this as an address for a given table). In this case we use it to generate an pointer for the table "all_tables" in schema "public".
- `dplyr::collect()` - the `tbl()` function generates a "lazy" table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. "do the shopping") we need to pipe in the `collect()` function.

**Also try:** Run the code above without `collect()`, to see what a lazy table looks like.

Now let's take a look at the table metadata to explore what schemas and tables exist.
```{r}
view(mdt)
```

### Column metadata
Suppose our interest is in the `survey_data` schema. Let's take a closer look at the tables here by collecting metadata on table columns in this schema.

```{r}
# load table "all_columns" from schema "public"
mdc = tbl(dbcon, Id("public", "all_columns")) %>%
  filter(table_schema == "survey_data") %>%
  collect()
```

Notice we used the `dplyr::filter()` command on the lazy table *before* running `collect()`. This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!

Let's check out the column metadata, and see what you can learn.

```{r}
view(mdc)

# list the columns in our column-metadata table
colnames(mdc)
```

Curious about what a certain metadata column means? There's metadata for that (metametadata?)!

```{r}
# vew metadata on metadata columns
view(mdc %>% filter(table_name == "metadata_columns"))

```

A few columns to point out:

- definition
- units
- data_type
- natural key

(more on keys later)

## Our first(?) data table
Ok, let's try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).
```{r}
# create lazy table for ves (visual encounter survey) table
db_ves = tbl(dbcon, Id("survey_data", "ves"))

```
Do these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn't look like data yet. Let's see what we can learn from it before going to the store to collect the data.

What columns the table contains:

```{r}
# return columns of lazy table
colnames(db_ves)

```

How many total rows a table contains:

```{r}
# count rows
db_ves %>%
  summarise(row_count = n()) %>%
  pull(row_count)

```
*The pull() function executes a query to return a single column or variable, synonymous with the collect() function which returns a collection of variables as a table.*

How many rows after filtering for unknown species:

```{r}
# count rows with known species
db_ves %>%
  filter(!is.na(species_ves),
         species_ves != "unknown_species") %>%
  summarise(row_count = n()) %>%
  pull(row_count)

```

How many rows corresponding to a each life stage:

```{r}
# count rows by life stage
db_ves %>%
  select(life_stage) %>%
  group_by(life_stage) %>%
  summarise(row_count = n()) %>%
  arrange(desc(row_count)) %>%
  collect()

```

## Disconnect
Reinforcing best practice by disconnecting from the server.

```{r}
dbDisconnect(dbcon)
```


# Python
Let's set up our environment to get ready to explore the database.

## Load packages

```{python, message=FALSE}
# minimal packages for Python DB data discovery
import ibis
from ibis import _
import pandas as pd
import dbconfig

```

## Establish database connection

```{python}
# Establish database connection
dbcon = ibis.postgres.connect(**dbconfig.ribbitr)
```

## Load database metadata
### Data structure: Schemas, tables, columns and rows
The RIBBiTR database is organized into "schemas" (think of these as folders), which can contain any number of tables. Each table consists of columns ("variables") and rows ("entries").

### Metadata: Data about data
We keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let's learn what tables are present in the data by loading the table metadata.

### Table Metadata

```{python}
# load table "all_tables" from schema "public"
mdt = dbcon.table(database = "public", name = "all_tables").to_pandas()
```

#### Some basic database commands

Before we take a look at the metadata you just pulled, let's understand the command we just ran.

- `ibis.table()` - This function is used to create a "lazy" table from a data source. To specify the source, we modify the database connection `dbcon`. We specify the schema for the table as `public` (note ibis calls this "database"), as well as the table name `all_tables`. A "lazy" table means that the data only pulled when explicitly asked for. See `execute()` below.
- `ibis.to_pandas()` - the `table()` function generates a "lazy" table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. "do the shopping") we need to collect the lazy table by chaining the `to_pandas()` function.

**Also try:** Run the code above without `to_pandas()`, to see what an uncollected lazy table looks like.

Now let's take a look at the table metadata to explore what schemas and tables exist.
```{python}
print(mdt)
```

### Column metadata
Suppose our interest is in the `survey_data` schema. Let's take a closer look at the tables here by collecting metadata on table columns in this schema.

```{python}
# load table "all_columns" from schema "public"
mdc = (
  dbcon.table(database="public", name="all_columns")
  .filter(_.table_schema == 'survey_data')
  .to_pandas()
)
```

Notice we used the `ibis.filter()` command on the lazy table *before* calling `to_pandas()`. This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!

Let's check out the column metadata, and see what you can learn.

```{python}
# view dataframe
print(mdc)

# list the columns in our column-metadata table
mdc.columns
```

Curious about what a certain metadata column means? There's metadata for that (metametadata?)!

```{python}
# view metadata on metadata columns
metameta = mdc[mdc['table_name'] == 'metadata_columns']
print(metameta)
```

A few columns to point out:

- definition
- units
- data_type
- natural key

(more on keys later)

## Our first(?) data table
Ok, let's try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).
```{python}
# create lazy table for ves (visual encounter survey) table
db_ves = dbcon.table(database="survey_data", name="ves")

```
Do these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn't look like data yet. Let's see what we can learn from it before going to the store to collect the data.

What columns the table contains:

```{python}
# return columns of lazy table
db_ves.columns

```

How many total rows a table contains:

```{python}
# count rows
(db_ves
 .count()
 .execute())

```
*The ibis.execute() function executes a query and returns the result, regardless of the format. This is synonymous with the to_pandas() function which returns query results as a pandas dataframe where possible.*

How many rows after filtering for unknown species:

```{python}
# count rows with known species
filtered_row_count = (
  db_ves
  .filter(_.species_ves.notnull() & (_.species_ves != 'unknown_species'))
  .count()
  .execute())

print(filtered_row_count)

```

How many rows corresponding to a each life stage:

```{python}
# count rows by life stage
life_stage_counts = (
    db_ves.group_by('life_stage')
    .aggregate(row_count=_.count())
    .order_by(_.row_count.desc())
    .to_pandas()
)

print(life_stage_counts)

```

## Disconnect
Reinforcing best practice by disconnecting from the server.

```{python}
# close connection
dbcon.disconnect()
```

# DBeaver
Double-click on the `ribbitr` connection in the "Database Navigator" panel to begin your connection. Once connected you should be able to navigae a dropdown menu to explore the connection.

## Load database metadata
### Data structure: Schemas, tables, columns and rows
The RIBBiTR database is organized into "schemas" (think of these as folders), which can contain any number of tables. Each table consists of columns ("variables") and rows ("entries"). You can explore this structure through the dropdown menu in the "Database Navigator" panel on the left.

![Navigate to `Databases` -> `ribbitr` -> `Schemas`](images/DBeaver_data_discovery_01.png)

### Metadata: Data about data
We keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let's learn what tables are present in the data by loading the table metadata.

### Table Metadata

![Navigate to `Databases` -> `ribbitr` -> `Schemas` -> `public` -> `Views` -> `all_tables` (double-click). Select the `Data` tab.](images/DBeaver_data_discovery_02.png)
See what you can learn about the tables in the database form the table metadata.

### Column metadata
Suppose our interest is in the `survey_data` schema. Let's take a closer look at the tables here by collecting metadata on table columns in this schema.

![Navigate to `Databases` -> `ribbitr` -> `Schemas` -> `public` -> `Views` -> `all_columns` (double-click). Select the `Data` tab.](images/DBeaver_data_discovery_03.png)
Click on the dropdown arrow next to `table_schema`, click on `Order by table_schema ASC`. Repeat for the `table_name` and `column_name` columns.

Scroll down until you see rows with `table_schema` = `survey_data`. Explore a table of interest t see what you can learn.

Curious about what a certain metadata column means? There's metadata for that (metametadata?)! Scroll down to `table_name` =  `metadata_columns` to learn what the different columns in the current table mean. 

A few columns to point out:

- definition
- units
- data_type
- natural key

(more on keys later)

## Schema sructure
![Navigate to `Databases` -> `ribbitr` -> `Schemas` -> `survey_data`. Right-click and select `View Schema`. Select the `ER Diagram` tab.](images/DBeaver_data_discovery_04.png)

This shows a diagram of the different tables within the `surevy_data` schema, as well as their columns and any relationships between tables. This is a useful visual reference for later, when we begin joining tables.

## Our first data table

To begin looking at data, let's navigate to the visual encounter surveys (VES).

![Navigate to `Databases` -> `ribbitr` -> `Schemas` -> `survey_data` -> `Tables` -> `ves` (double-click). Select the `Data` tab.](images/DBeaver_data_discovery_05.png)
This is your first look at field data within the database! From here you can explore organizing the data by columns, as well as exporting the table to a .csv.

:::
<div style="text-align: center;">
[Previous Tutorial: Connection Setup](01_connection_setup.html) | [Next Tutorial: Data Pulling](03_data_pulling.html)
</div>