---
title: "3. Data Pulling"
author: "Cob Staines"
date: today
toc: true
format:
  html: 
    code-line-numbers: false
  pdf:
    code-line-numbers: false
    geometry: 
      - top=30mm
      - left=30mm
---

```{r, include=FALSE}
librarian::shelf(reticulate)
# use_python("/home/cob/miniconda3/envs/ribbitr/bin/python")
use_condaenv("ribbitr", required = TRUE)

```

*This tutorial is available as a [.qmd on Github](https://github.com/RIBBiTR-BII/ribbitr-data-access/tree/main/tutorial_series).*

# Motivation
- Download or "pull" data from the database to our local machine
- Pre- select and filter to data of interest

::: {.panel-tabset}

# R
Let's set up our environment to get ready to pull data.

## Load packages

```{r, message=FALSE}
# minimal packages for RIBBiTR DB data discovery
librarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)
```

## Establish database connection

```{r}
# establish database connection
dbcon <- hopToDB("ribbitr")
```

## Load metadata
We recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).

```{r}
# load table "all_tables" from schema "public"
mdt <- tbl(dbcon, Id("public", "all_tables")) %>%
  collect()

# load table "all_columns" from schema "public", filtering to schema "survey_data"
mdc <- tbl(dbcon, Id("public", "all_columns")) %>%
  filter(table_schema == "survey_data") %>%
  collect()
```

## Pulling data
Let's construct our first data query, building from the previous tutorial.

```{r}
# lazy table and collect
db_ves <- tbl(dbcon, Id("survey_data", "ves")) %>%
  collect()
```

Great, that was easy! But what if we don't need all that data? Suppose we are only interested in certain columns? We can `dplyr::select()` for specific columns to avoid pulling unnecessary data:

```{r}
# lazy table, select, and collect
db_ves <- tbl(dbcon, Id("survey_data", "ves")) %>%
  select(taxon_ves,
         count_ves,
         life_stage,
         sex,
         survey_id) %>%
  collect()
```

And perhaps we are only interested in adults, in which case we can also `dplyr::filter()` our table to desired rows before collecting:

```{r}
# lazy table select, filter, and collect all in one
db_ves_adult <- tbl(dbcon, Id("survey_data", "ves")) %>%
  select(taxon_ves,
         count_ves,
         life_stage,
         sex,
         survey_id) %>%
  filter(life_stage == "adult") %>%
  collect()

# preview table
head(db_ves_adult)
```

Great! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.

## SQL aside
"Wait a minute... I thought these data were encoded in SQL? Where is the SQL?" Turns out, the package `dbplyr` does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.

But if we want to see the SQL, we can! Let's take a closer look at the lazy table for our last query (dropping the `collect()` statement):

```{r}
# lazy table only (not collected)
ves_adult <- tbl(dbcon, Id("survey_data", "ves")) %>%
  select(taxon_ves,
         count_ves,
         life_stage,
         sex,
         survey_id) %>%
  filter(life_stage == "adult")

# render sql from lazy table
(ves_adult_q = sql_render(ves_adult))

```
The `dbplyr::sql_render()` function converts our lazy table "shopping list" into an SQL script. If we want we can interact with this script, and even send it to the database manually using `DBI::dbGetQuery()`

```{r}
# execute SQL statement and return results
db_ves_adult_sql <- dbGetQuery(dbcon, ves_adult_q)

# preview table
head(db_ves_adult_sql)
```
On close inspection we see that this is identical to the db_ves_adult_sql above. Now you have two ways to access the same data!

We will stick with the dplyr/dbplyr methods for the rest of this tutorial, but feel free to integrate this with your curiousity and/or knowledge of SQL as we go forward.

## Disconnect

```{r}
dbDisconnect(dbcon)
```

## Also try:
- Check out the SQL code for your last query with:

```{r, eval=FALSE}
sql_render(db_ves_adult_final)
```

# Python
Let's set up our environment to get ready to pull data.

## Load packages

```{python, message=FALSE}
# minimal packages for RIBBiTR DB data discovery
import ibis
from ibis import _
import pandas as pd
import dbconfig
```

## Establish database connection

```{python}
# establish database connection
dbcon = ibis.postgres.connect(**dbconfig.ribbitr)
```

## Load metadata
We recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).

```{python}
# load table "all_tables" from schema "public"
mdt = dbcon.table(database = "public", name = "all_tables").to_pandas()

# load table "all_columns" from schema "public", filtering to schema "survey_data"
mdc = (
  dbcon.table(database="public", name="all_columns")
  .filter(_.table_schema == 'survey_data')
  .to_pandas()
)
```

## Pulling data
Let's construct our first data query, building from the previous tutorial.

```{python}
# lazy table and collect
db_ves = dbcon.table(database="survey_data", name="ves").to_pandas()
db_ves.columns
```

Great, that was easy! But what if we don't need all that data? Suppose we are only interested in certain columns? We can `ibis.select()` for specific columns to avoid pulling unnecessary data:

```{python}
# lazy table, select, and collect
db_ves_select = (
  dbcon.table(database="survey_data", name="ves")
  .select([
    'taxon_ves',
    'count_ves',
    'life_stage',
    'sex',
    'survey_id'
  ])
  .to_pandas()
  )
  
db_ves_select.columns
```

And perhaps we are only interested in adults, in which case we can also `ibis.filter()` our table to desired rows before collecting:

```{python}
# lazy table select, filter, and collect all in one
db_ves_adult = (
  dbcon.table(database="survey_data", name="ves")
  .select([
    'taxon_ves',
    'count_ves',
    'life_stage',
    'sex',
    'survey_id'
  ])
  .filter(_.life_stage == 'adult')
  .to_pandas()
  )

# preview table
db_ves_adult.head()
```

Great! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.

## SQL aside
"Wait a minute... I thought these data were encoded in SQL? Where is the SQL?" Turns out, the package `ibis` does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.

But if we want to see the SQL, we can! Let's take a closer look at the lazy table for our last query (dropping the `to_pandas()` statement):

```{python}
# lazy table only (not collected)
ves_adult = (
  dbcon.table(database="survey_data", name="ves")
  .select([
    'taxon_ves',
    'count_ves',
    'life_stage',
    'sex',
    'survey_id'
  ])
  .filter(_.life_stage == 'adult')
  )

# render sql from lazy table
ves_adult.compile()

```
The `ibis.compile()` function converts our lazy table "shopping list" into an SQL script. If we want we can revise with this script, and even send it to the database manually using dedicated python - SQL packages such as psycopg2 or SQLAlchemy.

## Disconnect
```{python}
# close connection
dbcon.disconnect()
```

## Also try:
- Check out the SQL code for your last query with:

```{python, eval=FALSE}
db_ves_adult_final.compile()
```

:::
<div style="text-align: center;">
[<- 2. Data Discovery](02_data_discovery.html) | [4. Table Joins ->](04_table_joins.html)
</div>