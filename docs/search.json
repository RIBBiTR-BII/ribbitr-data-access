[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RIBBiTR Database Access Center",
    "section": "",
    "text": "This is a repository for organizing and hosting data access resources within the RIBBiTR project. As an interdisciplinary and cross-scale research project, RIBBiTR’s goals require open collaboration and communication within the team.\nTake a look at the resources provided in the menu for this page.\nFacilitating the effective communication of data within the team is part of this effort, though is no replacement for interpersonal communication. Confused by the data? See something odd? Please reach out! You can help to enhance the communication and collaboration within our team. Contact the RIBBiTR data manager to talk it through."
  },
  {
    "objectID": "db_changelog.html",
    "href": "db_changelog.html",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Dropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.15 to 16.3\n\n\n\n\n\n\n\n\nRenamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit\n\n\n\n\n\n\n\n\n\nUnique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema\n\n\n\n\n\n\n\n\nchangelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).\n\n\n\n\n\n\n\n\n\nSet “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section",
    "href": "db_changelog.html#section",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Dropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-1",
    "href": "db_changelog.html#section-1",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.15 to 16.3",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-2",
    "href": "db_changelog.html#section-2",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Renamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-3",
    "href": "db_changelog.html#section-3",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Unique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-4",
    "href": "db_changelog.html#section-4",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-5",
    "href": "db_changelog.html#section-5",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "changelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-6",
    "href": "db_changelog.html#section-6",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Set “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html",
    "href": "tutorial_series/04_data_workflow.html",
    "title": "4. Data Workflow",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#setup",
    "href": "tutorial_series/04_data_workflow.html#setup",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n\n# establish database connection\ndbcon = hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n# load table metadata\nmdt = tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\n# load column metadata\nmdc = tbl(dbcon, Id(\"survey_data\", \"metadata_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling",
    "href": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nPull capture table\n\n# capture table, select, filter\ndb_capture = tbl(dbcon, Id(\"survey_data\", \"capture\")) %&gt;%\n  select(species_capture,\n         body_temp_c,\n         life_stage,\n         sex,\n         capture_animal_state,\n         bd_swab_id,\n         survey_id)\n\n\n\nJoin with support tables, filter by date\n\n# create chain object\nchain_capture = tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_chain = tbl_join(dbcon, chain_capture, tbl = db_capture) %&gt;%\n  filter(date &gt;= \"2015-01-01\")\n\nJoining with bd_swab_lookup ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nJoin with Bd table\n\ndb_bd_results = tbl(dbcon, Id(\"survey_data\", \"bd_qpcr_results\")) %&gt;%\n  select(bd_swab_id,\n         detected,\n         average_target_quant)\n\ndb_capture_bd = db_capture_chain %&gt;%\n  inner_join(db_bd_results, by=\"bd_swab_id\")\n\n\n\nExplore # of samples by life stage, then filter\n\ndb_capture_bd %&gt;%\n  select(life_stage) %&gt;%\n  group_by(life_stage) %&gt;%\n  summarise(row_count = n()) %&gt;%\n  arrange(desc(row_count)) %&gt;%\n  collect()\n\n# A tibble: 13 × 2\n   life_stage              row_count\n   &lt;chr&gt;                     &lt;int64&gt;\n 1 adult                       22746\n 2 &lt;NA&gt;                         4515\n 3 juvenile                     3937\n 4 tadpole                      1666\n 5 subadult                      984\n 6 aquatic_larvae                573\n 7 metamorph                     428\n 8 larva                         417\n 9 unknown                       367\n10 terrestrial_development       330\n11 larvae                        218\n12 eggmass                         7\n13 Unknown                         2\n\ndb_capture_bd_life = db_capture_bd %&gt;%\n  filter(life_stage %in% c(\"juvenile\",\n                           \"subadult\",\n                           \"adult\"),\n         !is.na(life_stage))\n\n\n\nExplore # of samples by species, then filter\n\n(spp_summary = db_capture_bd_life%&gt;%\n  select(species_capture) %&gt;%\n  group_by(species_capture) %&gt;%\n  summarise(sample_count = n()) %&gt;%\n  arrange(desc(sample_count)) %&gt;%\n  collect())\n\n# A tibble: 131 × 2\n   species_capture           sample_count\n   &lt;chr&gt;                          &lt;int64&gt;\n 1 rana_muscosa                     13328\n 2 rana_clamitans                    2366\n 3 rana_catesbeiana                  1601\n 4 pseudacris_crucifer               1118\n 5 lithobates_sphenocephalus          965\n 6 rana_pipiens                       551\n 7 notophthalmus_viridescens          535\n 8 lithobates_chiricahuensis          453\n 9 colostethus_panamensis             429\n10 hyla_versicolor                    417\n# ℹ 121 more rows\n\n(spp_list = spp_summary %&gt;%\n  filter(sample_count &gt;= 100) %&gt;%\n  pull(species_capture))\n\n [1] \"rana_muscosa\"              \"rana_clamitans\"           \n [3] \"rana_catesbeiana\"          \"pseudacris_crucifer\"      \n [5] \"lithobates_sphenocephalus\" \"rana_pipiens\"             \n [7] \"notophthalmus_viridescens\" \"lithobates_chiricahuensis\"\n [9] \"colostethus_panamensis\"    \"hyla_versicolor\"          \n[11] \"hyla_chrysoscelis\"         \"anaxyrus_americanus\"      \n[13] \"silverstoneia_flotator\"    \"lithobates_warszewitschii\"\n[15] \"anaxyrus_fowleri\"          \"acris_blanchardi\"         \n[17] \"rhaebo_haematiticus\"       \"sachatamia_albomaculata\"  \n[19] \"ambystoma_opacum\"          \"hyla_cinerea\"             \n[21] \"lithobates_sylvaticus\"     \"pseudacris_feriarum\"      \n[23] \"smilisca_sila\"             \"plethodon_glutinosis\"     \n[25] \"ambystoma_maculatum\"       \"plethodon_cinereus\"       \n[27] \"unknown_species\"           \"desmognathus_fuscus\"      \n[29] \"lithobates_blairi\"        \n\ndb_capture_bd_life_spp = db_capture_bd_life %&gt;%\n  filter(species_capture %in% spp_list,\n         !is.na(species_capture))\n\n\n\ncollect data\n\ndata_capture_bd_query = db_capture_bd_life_spp %&gt;%\n  collect()\n\ncolnames(data_capture_bd_query)\n\n [1] \"species_capture\"      \"body_temp_c\"          \"life_stage\"          \n [4] \"sex\"                  \"capture_animal_state\" \"bd_swab_id\"          \n [7] \"survey_id\"            \"detection_type\"       \"visit_id\"            \n[10] \"date\"                 \"survey_time\"          \"site_id\"             \n[13] \"site\"                 \"region_id\"            \"region\"              \n[16] \"country_id\"           \"country_name\"         \"detected\"            \n[19] \"average_target_quant\"\n\nhead(data_capture_bd_query)\n\n# A tibble: 6 × 19\n  species_capture   body_temp_c life_stage sex   capture_animal_state bd_swab_id\n  &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;     \n1 lithobates_warsz…        18.6 juvenile   unkn… alive                150607_c01\n2 rhaebo_haematiti…        NA   juvenile   unkn… alive                150608_04 \n3 rhaebo_haematiti…        NA   juvenile   unkn… alive                150608_05 \n4 colostethus_pana…        NA   adult      unkn… alive                150610_01 \n5 rhaebo_haematiti…        26.7 adult      unkn… alive                150612_01 \n6 rhaebo_haematiti…        25.7 juvenile   unkn… alive                150612_02 \n# ℹ 13 more variables: survey_id &lt;chr&gt;, detection_type &lt;chr&gt;, visit_id &lt;chr&gt;,\n#   date &lt;date&gt;, survey_time &lt;chr&gt;, site_id &lt;chr&gt;, site &lt;chr&gt;, region_id &lt;chr&gt;,\n#   region &lt;chr&gt;, country_id &lt;chr&gt;, country_name &lt;chr&gt;, detected &lt;dbl&gt;,\n#   average_target_quant &lt;dbl&gt;\n\n\nThese data are ready to be analyzed and visualized!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#setup-1",
    "href": "tutorial_series/04_data_workflow.html#setup-1",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB Workflow\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig\nimport db_access as db\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\n# load table metadata\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load column metadata\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n  )",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling-1",
    "href": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling-1",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nPull capture table\n\n# capture table, select, filter\ndb_capture = (\n  dbcon.table(database=\"survey_data\", name=\"capture\")\n  .select([\n    'species_capture',\n    'body_temp_c',\n    'life_stage',\n    'sex',\n    'capture_animal_state',\n    'bd_swab_id',\n    'survey_id'\n    ])\n  )\n\n\n\nJoin with support tables, filter to date\n\n# create chain object\nchain_capture = db.tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_chain = (\n  db.tbl_join(dbcon, chain_capture, tbl=db_capture)\n  .filter(_.date &gt;= '2015-01-01')\n  )\n\nJoining with bd_swab_lookup ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nJoin with Bd table\n\n# bd qpcr results lazy table\ndb_bd_results = (\n  dbcon.table(database=\"survey_data\", name=\"bd_qpcr_results\")\n  .select([\n    'bd_swab_id',\n    'detected',\n    'average_target_quant'\n  ])\n  )\n\n# join capture and bd tables\ndb_capture_bd = (\n  db_capture_chain\n  .inner_join(db_bd_results, db_capture_chain.bd_swab_id == db_bd_results.bd_swab_id)\n  )\n\n\n\nExplore # of samples by life stage, then filter\n\n# count by life stage\nlife_stage_counts = (\n  db_capture_bd\n  .group_by('life_stage')\n  .aggregate(row_count=_.count())\n  .order_by(_.row_count.desc())\n  .to_pandas()\n  )\n\nprint(life_stage_counts)\n\n                 life_stage  row_count\n0                     adult      22746\n1                      None       4515\n2                  juvenile       3937\n3                   tadpole       1666\n4                  subadult        984\n5            aquatic_larvae        573\n6                 metamorph        428\n7                     larva        417\n8                   unknown        367\n9   terrestrial_development        330\n10                   larvae        218\n11                  eggmass          7\n12                  Unknown          2\n\n\n# filter to desired life stages\ndb_capture_bd_life = (\n  db_capture_bd\n  .filter(_.life_stage.isin(['juvenile','subadult', 'adult']) & _.life_stage.notnull())\n  )\n\n\n\nExplore # of samples by species, then filter\n\n# count by species\nspp_summary = (\n  db_capture_bd_life\n  .group_by('species_capture')\n  .aggregate(sample_count=_.count())\n  .order_by(_.sample_count.desc())\n  .to_pandas()\n  )\nprint(spp_summary)\n\n                  species_capture  sample_count\n0                    rana_muscosa         13328\n1                  rana_clamitans          2366\n2                rana_catesbeiana          1601\n3             pseudacris_crucifer          1118\n4       lithobates_sphenocephalus           965\n..                            ...           ...\n126             silverstoneia_spp             1\n127                 diasporus_spp             1\n128        craugastor_monnichorum             1\n129               acris_crepitans             1\n130  hyalinobatrachium_talamancae             1\n\n[131 rows x 2 columns]\n\n\n# generate species list\nspp_list = spp_summary[spp_summary['sample_count'] &gt;= 100]['species_capture'].tolist()\n\n# filter to species in spp_list\ndb_capture_bd_life_spp = (\n  db_capture_bd_life\n  .filter(_.species_capture.isin(spp_list) & _.species_capture.notnull())\n  )\n\n\n\nPull data\n\n# pull data\ndata_capture_bd_query = db_capture_bd_life_spp.to_pandas()\n\n# printcolumn names\ndata_capture_bd_query.columns\n\nIndex(['species_capture', 'body_temp_c', 'life_stage', 'sex',\n       'capture_animal_state', 'bd_swab_id', 'survey_id', 'bd_swab_id_right',\n       'survey_id_right', 'visit_id', 'detection_type', 'visit_id_right',\n       'site_id', 'date', 'survey_time', 'region_id', 'site_id_right', 'site',\n       'country_id', 'region_id_right', 'region', 'country_name',\n       'country_id_right', 'detected', 'average_target_quant'],\n      dtype='object')\n\n# preview data\ndata_capture_bd_query.head()\n\n             species_capture  body_temp_c  ... detected average_target_quant\n0        rhaebo_haematiticus          NaN  ...      0.0                 0.00\n1  lithobates_warszewitschii         18.6  ...      1.0              1007.69\n2  lithobates_warszewitschii          NaN  ...      1.0               257.43\n3  lithobates_warszewitschii         21.8  ...      1.0                56.15\n4  lithobates_warszewitschii         21.6  ...      0.0                 0.00\n\n[5 rows x 25 columns]\n\n\nThese data are ready to be analyzed and visualized!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html",
    "href": "tutorial_series/01_connection_setup.html",
    "title": "1. Connection Setup",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters",
    "href": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters",
    "title": "1. Connection Setup",
    "section": "Store & access your database connection parameters",
    "text": "Store & access your database connection parameters\n\nAccess your local .Renviron file\nYour .Renviron file a local file where you can save and reference your login credentials for easy use within R and RStudio, without risking losing them or potentially sharing them on accident when you share your code. A simple way to access your .Renviron file is with the function usethis::edit_r_environ()\n\ninstall.packages(\"usethis\")\n\n# open your local .Reniron file\nusethis::edit_r_environ()\n\n\n\nSave connections parameters\nCopy the following database connection parameters to your .Renviron file, substituting your login credentials (user & password).\n\n# RIBBiTR DB credentials\nribbitr.dbname = \"ribbitr\"\nribbitr.host = \"ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com\"\nribbitr.port = \"5432\"\nribbitr.user = \"[YOUR-USERNAME-HERE]\"\nribbitr.password = \"[YOUR-PASSWORD-HERE]\"\n\nSave and close .Renviron, and restart RStudio.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#establish-database-connection",
    "href": "tutorial_series/01_connection_setup.html#establish-database-connection",
    "title": "1. Connection Setup",
    "section": "Establish database connection",
    "text": "Establish database connection\nCreate a new R project (or .qmd, .Rmd, .R etc.) file where you can follow the tutorial and establish the database connection.\n\nLoad packages\n\nRtools\nRtools is required to build R packages locally, as part of this tutorial. Check to see if Rtools is installed by running:\n\nSys.which(\"make\")\n\n           make \n\"/usr/bin/make\" \n\n\nIf this returns a path to the make function (e.g. /usr/bin/make), Rtools is installed and you can proceed to the next step. If you return an empty string \"\" you will need to download Rtools first.\n\n\nLibrarian\n“librarian” is a package and library management package in R which makes it easier to install, load, update and unload packages to meet dynamic environment needs. There are other ways to download, load, and maintain packages in R (e.g. install.packages() and library(), but we recommend librarian for its simplicity and portability.\n\n# install and load \"librarian\" R package\ninstall.packages(\"librarian\")\n\nlibrarian downloads and loads packages using the librarian::shelf function. Below are the minimal recommended packages to establish a connection to the RIBBiTR database.\n\n# minimal packages for establishing RIBBiTR DB connection\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n# librarian::shelf(RIBBiTR-BII/ribbitrrr, update_all = TRUE)\n\n\n\n\nConnect\nNow, using the ribbitrrr:hopToDB() function, let’s establish a connection!\n\n# establish database connection\ndbcon = hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n\nhopToDB() returns a database connection object (dbcon). Keep track of this, you will call it to explore and pull data later.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#begin-using-your-connection",
    "href": "tutorial_series/01_connection_setup.html#begin-using-your-connection",
    "title": "1. Connection Setup",
    "section": "Begin using your connection!",
    "text": "Begin using your connection!\nTry out your connection by loading table metadata from the database\n\nmdt = tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\nhead(mdt)\n\n# A tibble: 6 × 4\n  table_schema table_name         column_count table_description\n  &lt;chr&gt;        &lt;chr&gt;                   &lt;int64&gt; &lt;chr&gt;            \n1 bay_area     amphib_dissect               41 &lt;NA&gt;             \n2 bay_area     amphib_parasite              11 &lt;NA&gt;             \n3 bay_area     water_quality_info           27 &lt;NA&gt;             \n4 bay_area     site                         25 &lt;NA&gt;             \n5 bay_area     wetland_info                 25 &lt;NA&gt;             \n6 bay_area     bd_results                   25 &lt;NA&gt;",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#also-try",
    "href": "tutorial_series/01_connection_setup.html#also-try",
    "title": "1. Connection Setup",
    "section": "Also try",
    "text": "Also try\n\nFor those managing multiple database connections, the hopToDB() function allows you to store and fetch various sets of login credentials with a single keyword. Just substitute “ribbitr” in the .Renviron example above with your own keywords to juggle multiple logins.\nYour login credentials can also be accessed explicitly anytime using Sys.getenv(\"ribbitr.dbname\"), etc. In most cases the hopToDB() function is all you will need, however.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-1",
    "href": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-1",
    "title": "1. Connection Setup",
    "section": "Store & access your database connection parameters",
    "text": "Store & access your database connection parameters\n\nCreate a dbconfig file\nWe recommend you create a local database config (dbconfig.py) file where you can save and reference your login credentials for easy use in python, without risking losing them or potentially sharing them on accident when you share your code.\nCreate a file nammed dbconfig.py in your project working directory (or another preferred location, see “Also try” below). Copy the following to dbconfig.py:\n\n# dbconfig.py\n\nribbitr = {\n  \"database\":\"ribbitr\",\n  \"host\":\"ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com\",\n  \"port\":\"5432\",\n  \"user\":\"[YOUR-USERNAME-HERE]\",\n  \"password\":\"[YOUR-PASSWORD-HERE]\",\n}\n\nSave dbconfig.py.\nBe sure to add dbconfig.py to your local .gitignore file if you are using git/github, so you don’t accidentally publish you login credentials!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#establish-database-connection-1",
    "href": "tutorial_series/01_connection_setup.html#establish-database-connection-1",
    "title": "1. Connection Setup",
    "section": "Establish database connection",
    "text": "Establish database connection\nCreate a new .py (or .qmd, .ipynb, etc.) file where you can follow the tutorial and establish the database connection.\n\nImport packages\nThis method requires installing the ibis.postgres package to your working environment, in addition to pandas. We also import the dbconfig.py file to access your login credentials.\n\nimport ibis\nimport pandas as pd\nimport dbconfig  # import connection credentials\n\n\n\nConnect\nNow, using the ibis.postgres.connect() function, let’s establish a connection!\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\nibis.postgres.connect() returns a database connection object (dbcon). Keep track of this, you will call it to explore and pull data later.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#begin-using-your-connection-1",
    "href": "tutorial_series/01_connection_setup.html#begin-using-your-connection-1",
    "title": "1. Connection Setup",
    "section": "Begin using your connection",
    "text": "Begin using your connection\nTry out your connection by loading table metadata from the database\n\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\nmdt.head()\n\n  table_schema          table_name  column_count table_description\n0     bay_area      amphib_dissect            41              None\n1     bay_area     amphib_parasite            11              None\n2     bay_area  water_quality_info            27              None\n3     bay_area                site            25              None\n4     bay_area        wetland_info            25              None",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#also-try-1",
    "href": "tutorial_series/01_connection_setup.html#also-try-1",
    "title": "1. Connection Setup",
    "section": "Also try",
    "text": "Also try\n\nFor those managing multiple database connections, this method allows you to store and fetch various sets of login credentials with a single keyword. Just substitute “ribbitr” in the dbconfig.py file with your own keywords and call them as needed!\nIf you will be connecting to the database from different python projects, you may want to save your dbconfig.py file to a more general location. In this case, include the following lines in each of your project files:\n\n\nimport sys\nsys.path.append(\"/path/to/dbconfig/dir/\")\nimport dbconfig",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/00_tutorial_series.html",
    "href": "tutorial_series/00_tutorial_series.html",
    "title": "RIBBiTR Database Tutorial Series",
    "section": "",
    "text": "Welcome!\nThis is a tutorial series for the RIBBiTR Database. Working through the tutorials on the left, you can work through examples of how to connect to the database, explore what data is there and unerstand it’s structure, efficiently download the data of interest, and pass it to your workflows. This series includes examples in R and Python.\nBegin with 1. Connection Setup.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html",
    "href": "tutorial_series/03_data_pulling.html",
    "title": "3. Data Pulling",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-packages",
    "href": "tutorial_series/03_data_pulling.html#load-packages",
    "title": "3. Data Pulling",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#establish-database-connection",
    "href": "tutorial_series/03_data_pulling.html#establish-database-connection",
    "title": "3. Data Pulling",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon = hopToDB(\"ribbitr\")\n\nConnecting to database... Success!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-metadata",
    "href": "tutorial_series/03_data_pulling.html#load-metadata",
    "title": "3. Data Pulling",
    "section": "Load metadata",
    "text": "Load metadata\nWe recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).\n\n# load table \"all_tables\" from schema \"public\"\nmdt = tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\n\n# load table \"all_columns\" from schema \"public\", filtering to schema \"survey_data\"\nmdc = tbl(dbcon, Id(\"public\", \"all_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#pulling-data",
    "href": "tutorial_series/03_data_pulling.html#pulling-data",
    "title": "3. Data Pulling",
    "section": "Pulling data",
    "text": "Pulling data\nLet’s construct our first data query, building from the previous tutorial.\n\n# lazy table and collect\ndb_ves = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  collect()\n\nGreat, that was easy! But what if we don’t need all that data? Suppose we are only interested in certain columns? We can dplyr::select() for specific columns to avoid pulling unnecessary data:\n\n# lazy table, select, and collect\ndb_ves = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  collect()\n\nAnd perhaps we are only interested in adults, in which case we can also dplyr::filter() our table to desired rows before collecting:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\") %&gt;%\n  collect()\n\n# preview table\nhead(db_ves_adult)\n\n# A tibble: 6 × 5\n  species_ves            count_ves life_stage sex     survey_id                 \n  &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                     \n1 colostethus_panamensis         2 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n2 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n3 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n4 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n5 silverstoneia_flotator         1 adult      unkonwn fd70bfc9-693e-4a31-af06-5…\n6 silverstoneia_flotator         1 adult      unkonwn fd70bfc9-693e-4a31-af06-5…\n\n\nGreat! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#sql-aside",
    "href": "tutorial_series/03_data_pulling.html#sql-aside",
    "title": "3. Data Pulling",
    "section": "SQL aside",
    "text": "SQL aside\n“Wait a minute… I thought these data were encoded in SQL? Where is the SQL?” Turns out, the package dbplyr does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.\nBut if we want to see the SQL, we can! Let’s take a closer look at the lazy table for our last query (dropping the collect() statement):\n\n# lazy table only (not collected)\nves_adult = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\")\n\n# render sql from lazy table\n(ves_adult_q = sql_render(ves_adult))\n\n&lt;SQL&gt; SELECT \"species_ves\", \"count_ves\", \"life_stage\", \"sex\", \"survey_id\"\nFROM \"survey_data\".\"ves\"\nWHERE (\"life_stage\" = 'adult')\n\n\nThe dbplyr::sql_render() function converts our lazy table “shopping list” into an SQL script. If we want we can interact with this script, and even send it to the database manually using DBI::dbGetQuery()\n\n# execute SQL statement and return results\ndb_ves_adult_sql = dbGetQuery(dbcon, ves_adult_q)\n\n# preview table\nhead(db_ves_adult_sql)\n\n             species_ves count_ves life_stage     sex\n1 colostethus_panamensis         2      adult    &lt;NA&gt;\n2 colostethus_panamensis         1      adult    &lt;NA&gt;\n3 colostethus_panamensis         1      adult    &lt;NA&gt;\n4 colostethus_panamensis         1      adult    &lt;NA&gt;\n5 silverstoneia_flotator         1      adult unkonwn\n6 silverstoneia_flotator         1      adult unkonwn\n                             survey_id\n1 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n2 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n3 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n4 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n5 fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n6 fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n\n\nOn close inspection we see that this is identical to the db_ves_adult_sql above. Now you have two ways to access the same data!\nWe will stick with the dplyr/dbplyr methods for the rest of this tutorial, but feel free to integrate this with your curiousity and/or knowledge of SQL as we go forward.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#joins",
    "href": "tutorial_series/03_data_pulling.html#joins",
    "title": "3. Data Pulling",
    "section": "Joins",
    "text": "Joins\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country?”\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nNatural Key (nk or nkey) – A meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key.\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\n\nWhen we pull a data table from the database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have of column metadata to help us keep track of this! Check out the key_type and natural_key columns for the survey table:\n\nves_metadata = mdc %&gt;%\n  select(table_name,\n         column_name,\n         key_type,\n         natural_key) %&gt;%\n  filter(table_name == \"ves\")\n\nview(ves_metadata)\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there another way?\n\n## ribbitrrr key functions\n\n# primary key for ves table\ntbl_pkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# natural key for ves table\ntbl_nkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# foreign key for ves table\ntbl_fkey(\"ves\", mdc)\n\n[1] \"survey_id\"\n\n# all unique key columns for ves table\ntbl_keys(\"ves\", mdc)\n\n[1] \"ves_id\"    \"survey_id\"\n\n\nNotice that we passed the column metadata to these functions, to help us automate this otherwise tedious task.\n\n\nJoining manually by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example:\n\n# lazy table of ves\ndb_ves = tbl(dbcon, Id(\"survey_data\", \"ves\"))\n\n# lazy table of survey\ndb_survey = tbl(dbcon, Id(\"survey_data\", \"survey\"))\n\ndb_ves_survey = db_ves %&gt;%\n  left_join(db_survey, by=\"survey_id\")\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                      \n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with by = tbl_fkey(\"ves\", mdc).\nIn order to join the VES data with country, we will have to do several, recursive joins to connect the tables of… (consults schema diagram)… survey, visit, site, region, and country! Is there another way?",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#links-chains-automated-joins",
    "href": "tutorial_series/03_data_pulling.html#links-chains-automated-joins",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some functions to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = tbl_join(dbcon, link_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                      \n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there another way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = tbl_join(dbcon, chain_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                       \"date\"                          \n[71] \"survey_time\"                    \"campaign\"                      \n[73] \"visit_status\"                   \"comments_visit\"                \n[75] \"site_id\"                        \"site\"                          \n[77] \"utm_zone\"                       \"utme\"                          \n[79] \"utmn\"                           \"area_sqr_m\"                    \n[81] \"site_code\"                      \"elevation_m\"                   \n[83] \"depth_m\"                        \"topo\"                          \n[85] \"wilderness\"                     \"site_comments\"                 \n[87] \"region_id\"                      \"region\"                        \n[89] \"country_id\"                     \"location_id\"                   \n[91] \"country_name\"                  \n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in seperate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# lazy table, select, filter\ndb_ves_adult = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\")\n\n# create chain object\nchain_ves = tbl_chain(\"ves\", mdc)\n\n# join recursively, specifying desired columns, filter, collect\ndb_ves_adult_final = tbl_join(dbcon, chain_ves, tbl = db_ves_adult, columns = c(\"country_name\")) %&gt;%\n  filter(country_name == \"brazil\")\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# pull selected, filtered, joined data\ndata_ves_adult_final = db_ves_adult_final %&gt;%\n  collect()\n\nA few differences here:\n\nWe provided our pre-selected and filtered dv_ves_adult table to the join function with tbl = db_ves_adult, rather than having it pull all the data.\nWe specified any additional columns to include with columns = c(\"country_name\"), in addition to any key columns (included by default). The result is much less columns, only those we want.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#also-try",
    "href": "tutorial_series/03_data_pulling.html#also-try",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nRun ?tbl_chain and ?tbl_join to Learn more about other possible parameters to pass to these functions\nCheck out the SQL code for your last query with:\n\n\nsql_render(db_ves_adult_final)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-packages-1",
    "href": "tutorial_series/03_data_pulling.html#load-packages-1",
    "title": "3. Data Pulling",
    "section": "Load packages",
    "text": "Load packages\nFor the second half of this tutorial, you will need to download the db_access.py script and save it alongside your dbconfig.py file. This script will provide us with some useful functions for easily pulling data.\n\n# minimal packages for RIBBiTR DB data discovery\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig\nimport db_access as db",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#establish-database-connection-1",
    "href": "tutorial_series/03_data_pulling.html#establish-database-connection-1",
    "title": "3. Data Pulling",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-metadata-1",
    "href": "tutorial_series/03_data_pulling.html#load-metadata-1",
    "title": "3. Data Pulling",
    "section": "Load metadata",
    "text": "Load metadata\nWe recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).\n\n# load table \"all_tables\" from schema \"public\"\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load table \"all_columns\" from schema \"public\", filtering to schema \"survey_data\"\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#pulling-data-1",
    "href": "tutorial_series/03_data_pulling.html#pulling-data-1",
    "title": "3. Data Pulling",
    "section": "Pulling data",
    "text": "Pulling data\nLet’s construct our first data query, building from the previous tutorial.\n\n# lazy table and collect\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\").to_pandas()\ndb_ves.columns\n\nIndex(['species_ves', 'count_ves', 'detection_location', 'microhab',\n       'life_stage', 'sex', 'comments_ves', 'microhab_moredetail',\n       'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id'],\n      dtype='object')\n\n\nGreat, that was easy! But what if we don’t need all that data? Suppose we are only interested in certain columns? We can ibis.select() for specific columns to avoid pulling unnecessary data:\n\n# lazy table, select, and collect\ndb_ves_select = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .to_pandas()\n  )\n  \ndb_ves_select.columns\n\nIndex(['species_ves', 'count_ves', 'life_stage', 'sex', 'survey_id'], dtype='object')\n\n\nAnd perhaps we are only interested in adults, in which case we can also ibis.filter() our table to desired rows before collecting:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  .to_pandas()\n  )\n\n# preview table\ndb_ves_adult.head()\n\n              species_ves  ...                             survey_id\n0  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n1  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n2  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n3  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n4  silverstoneia_flotator  ...  fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n\n[5 rows x 5 columns]\n\n\nGreat! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#sql-aside-1",
    "href": "tutorial_series/03_data_pulling.html#sql-aside-1",
    "title": "3. Data Pulling",
    "section": "SQL aside",
    "text": "SQL aside\n“Wait a minute… I thought these data were encoded in SQL? Where is the SQL?” Turns out, the package ibis does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.\nBut if we want to see the SQL, we can! Let’s take a closer look at the lazy table for our last query (dropping the to_pandas() statement):\n\n# lazy table only (not collected)\nves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# render sql from lazy table\nves_adult.compile()\n\n'SELECT \"t0\".\"species_ves\", \"t0\".\"count_ves\", \"t0\".\"life_stage\", \"t0\".\"sex\", \"t0\".\"survey_id\" FROM \"survey_data\".\"ves\" AS \"t0\" WHERE \"t0\".\"life_stage\" = \\'adult\\''\n\n\nThe ibis.compile() function converts our lazy table “shopping list” into an SQL script. If we want we can revise with this script, and even send it to the database manually using dedicated python - SQL packages such as psycopg2 or SQLAlchemy.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#joins-1",
    "href": "tutorial_series/03_data_pulling.html#joins-1",
    "title": "3. Data Pulling",
    "section": "Joins",
    "text": "Joins\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country?”\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nNatural Key (nk or nkey) – A meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key.\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\n\nWhen we pull a data table from the database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have of column metadata to help us keep track of this! Check out the key_type and natural_key columns for the survey table:\n\nves_metadata = mdc[(mdc['table_name'] == 'ves')][['table_name', 'column_name', 'key_type', 'natural_key']]\n\nprint(ves_metadata)\n\n    table_name          column_name key_type natural_key\n98         ves               ves_id       PK        True\n124        ves  visual_animal_state     None       False\n326        ves         comments_ves     None       False\n331        ves            count_ves     None       False\n332        ves   detection_location     None       False\n333        ves           life_stage     None       False\n334        ves             microhab     None       False\n335        ves  microhab_moredetail     None       False\n336        ves         observer_ves     None       False\n337        ves                  sex     None       False\n338        ves          species_ves     None       False\n339        ves            survey_id       FK       False\n\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there not a better way?\n\n## data_access key functions\n\n# primary key for ves table\ndb.tbl_pkey(\"ves\", mdc)\n\n['ves_id']\n\n# natural key for ves table\ndb.tbl_nkey(\"ves\", mdc)\n\n['ves_id']\n\n# foreign key for ves table\ndb.tbl_fkey(\"ves\", mdc)\n\n['survey_id']\n\n# all unique key columns for ves table\ndb.tbl_keys(\"ves\", mdc)\n\n['survey_id', 'ves_id']\n\n\nNotice that we passed the column metadata to these functions, to help us automate this otherwise tedious task.\n\n\nJoining manually by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example:\n\n# ves lazy table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n# survey lazy table\ndb_survey = dbcon.table(database=\"survey_data\", name=\"survey\")\n# joined lazy table\ndb_ves_survey = db_ves.left_join(db_survey, \"survey_id\")\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with tbl_fkey(\"ves\", mdc).\nIn order to join the VES data with country, we will have to do several, recursive joins to connect the tables of… (consults schema diagram)… survey, visit, site, region, and country! Is there not a better way?",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#links-chains-automated-joins-1",
    "href": "tutorial_series/03_data_pulling.html#links-chains-automated-joins-1",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some functions to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = db.tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, link_ves, columns=\"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there not a better way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, chain_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id', 'date', 'survey_time', 'campaign', 'visit_status', 'comments_visit', 'visit_id_right', 'site_id', 'site', 'utm_zone', 'utme', 'utmn', 'area_sqr_m', 'site_code', 'elevation_m', 'depth_m', 'topo', 'wilderness', 'site_comments', 'site_id_right', 'region_id', 'region', 'region_id_right', 'country_id', 'location_id', 'country_name', 'country_id_right']\n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in seperate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# lazy table, select, filter\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n    ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# create chain object\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join recursively, providing selected and filtered table\ndb_ves_adult_final = (\n  db.tbl_join(dbcon, chain_ves, tbl=db_ves_adult)\n  .filter(_.country_name == 'brazil')\n  )\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n# pull selected, filtered, joined data\ndata_ves_adult_final = db_ves_adult_final.to_pandas()\n\nA few differences here:\n\nWe provided our pre-selected and filtered dv_ves_adult table to the join function with tbl = db_ves_adult, rather than having it pull all the data.\nWe specified any additional columns to include with columns = c(\"country_name), in addition to any key columns (included by default). The result is much less columns, only those we want.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#also-try-1",
    "href": "tutorial_series/03_data_pulling.html#also-try-1",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nCheck out the SQL code for your last query with:\n\n\ndb_ves_adult_final.compile()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "index.html#ribbitr-objectives",
    "href": "index.html#ribbitr-objectives",
    "title": "RIBBiTR Database Basics",
    "section": "RIBBiTR Objectives",
    "text": "RIBBiTR Objectives\nResearch goals\n\ncore questions (Q1-Q4) span\n\ntime scales\nspacial scales\nand therefore biological levels\n\n\nCommunity goals\n\nteam science, collaborative research\n\nno one person understands it all"
  },
  {
    "objectID": "index.html#ribbitr-data-sharing-plan",
    "href": "index.html#ribbitr-data-sharing-plan",
    "title": "RIBBiTR Database Basics",
    "section": "RIBBiTR Data Sharing Plan",
    "text": "RIBBiTR Data Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "index.html#ideal-solution",
    "href": "index.html#ideal-solution",
    "title": "RIBBiTR Database Basics",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "index.html#the-database-one-solution",
    "href": "index.html#the-database-one-solution",
    "title": "RIBBiTR Database Basics",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "index.html#how-it-works-kinda",
    "href": "index.html#how-it-works-kinda",
    "title": "RIBBiTR Database Basics",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "index.html#results-from-survey",
    "href": "index.html#results-from-survey",
    "title": "RIBBiTR Database Basics",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "index.html#closing-the-loop",
    "href": "index.html#closing-the-loop",
    "title": "RIBBiTR Database Basics",
    "section": "Closing the loop",
    "text": "Closing the loop\n\nProject goals: Data as connection & understanding (not data for data sake)\nCommunity goals: Data as participatory (communicating ad collaborating to lower barriers to access)"
  },
  {
    "objectID": "index.html#lets-practice",
    "href": "index.html#lets-practice",
    "title": "RIBBiTR Database Basics",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "index.html#basic-motivations-and-mechanisms-behind-the-server",
    "href": "index.html#basic-motivations-and-mechanisms-behind-the-server",
    "title": "RIBBiTR Database Basics",
    "section": "Basic motivations and mechanisms behind the server",
    "text": "Basic motivations and mechanisms behind the server"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-objectives",
    "href": "presentations/db_basics_presentation.html#ribbitr-objectives",
    "title": "RIBBiTR Database: Collaboration with Context",
    "section": "RIBBiTR Objectives",
    "text": "RIBBiTR Objectives\nResearch goals\n\ncore questions (Q1-Q4) span\n\ntime scales\nspacial scales\nand therefore biological levels\n\n\nCommunity goals\n\nteam science, collaborative research\n\nno one person understands it all"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-data-sharing-plan",
    "href": "presentations/db_basics_presentation.html#ribbitr-data-sharing-plan",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "RIBBiTR Data Sharing Plan",
    "text": "RIBBiTR Data Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ideal-solution",
    "href": "presentations/db_basics_presentation.html#ideal-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#the-database-one-solution",
    "href": "presentations/db_basics_presentation.html#the-database-one-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#how-it-works-kinda",
    "href": "presentations/db_basics_presentation.html#how-it-works-kinda",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#results-from-survey",
    "href": "presentations/db_basics_presentation.html#results-from-survey",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#closing-the-loop",
    "href": "presentations/db_basics_presentation.html#closing-the-loop",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Closing the loop",
    "text": "Closing the loop\n\nProject goals: Data as connection & understanding (not data for data sake)\nCommunity goals: Data as participatory (communicating ad collaborating to lower barriers to access)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#lets-practice",
    "href": "presentations/db_basics_presentation.html#lets-practice",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#basic-motivations-and-mechanisms-behind-the-server",
    "href": "presentations/db_basics_presentation.html#basic-motivations-and-mechanisms-behind-the-server",
    "title": "RIBBiTR Database: Collaboration with Context",
    "section": "Basic motivations and mechanisms behind the server",
    "text": "Basic motivations and mechanisms behind the server"
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html",
    "href": "tutorial_series/02_data_discovery.html",
    "title": "2. Data Discovery",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-packages",
    "href": "tutorial_series/02_data_discovery.html#load-packages",
    "title": "2. Data Discovery",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#establish-database-connection",
    "href": "tutorial_series/02_data_discovery.html#establish-database-connection",
    "title": "2. Data Discovery",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon = hopToDB(\"ribbitr\")\n\nConnecting to database... Success!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-database-metadata",
    "href": "tutorial_series/02_data_discovery.html#load-database-metadata",
    "title": "2. Data Discovery",
    "section": "Load database metadata",
    "text": "Load database metadata\n\nData structure: Schemas, tables, columns and rows\nThe RIBBiTR database is organized into “schemas” (think of these as folders), which can contain any number of tables. Each table consists of columns (“variables”) and rows (“entries”).\n\n\nMetadata: Data about data\nWe keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let’s learn what tables are present in the data by loading the table metadata.\n\n\nTable Metadata\n\n# load table \"all_tables\" from schema \"public\"\nmdt = tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\n\n\nSome basic database commands\nBefore we take a look at the metadata you just pulled, let’s understand the command we just ran.\n\ndplyr::tbl() - This function is used to create a “lazy” table from a data source. To specify the source, we provide the database connection dbcon, as well as a pointer or “address” for the table of interest using the Id() function. A “lazy” table means that the data only pulled when explicitly asked for. See collect() below.\ndbplyr::Id() - This function is a pointer to pass hierarchical table identifiers (you can think of this as an address for a given table). In this case we use it to generate an pointer for the table “all_tables” in schema “public”.\ndplyr::collect() - the tbl() function generates a “lazy” table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. “do the shopping”) we need to pipe in the collect() function.\n\nAlso try: Run the code above without collect(), to see what a lazy table looks like.\nNow let’s take a look at the table metadata to explore what schemas and tables exist.\n\nview(mdt)\n\n\n\n\nColumn metadata\nSuppose our interest is in the survey_data schema. Let’s take a closer look at the tables here by collecting metadata on table columns in this schema.\n\n# load table \"all_columns\" from schema \"public\"\nmdc = tbl(dbcon, Id(\"public\", \"all_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\nNotice we used the dplyr::filter() command on the lazy table before running collect(). This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!\nLet’s check out the column metadata, and see what you can learn.\n\nview(mdc)\n\n# list the columns in our column-metadata table\ncolnames(mdc)\n\n [1] \"table_schema\"             \"table_name\"              \n [3] \"column_name\"              \"definition\"              \n [5] \"units\"                    \"accuracy\"                \n [7] \"scale\"                    \"format\"                  \n [9] \"natural_key\"              \"reviewed\"                \n[11] \"data_type\"                \"character_maximum_length\"\n[13] \"numeric_precision\"        \"datetime_precision\"      \n[15] \"is_nullable\"              \"column_default\"          \n[17] \"ordinal_position\"         \"pg_description\"          \n[19] \"key_type\"                \n\n\nCurious about what a certain metadata column means? There’s metadata for that (metametadata?)!\n\n# vew metadata on metadata columns\nview(mdc %&gt;% filter(table_name == \"metadata_columns\"))\n\nA few columns to point out:\n\ndefinition\nunits\ndata_type\nnatural key\n\n(more on keys later)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#our-first-data-table",
    "href": "tutorial_series/02_data_discovery.html#our-first-data-table",
    "title": "2. Data Discovery",
    "section": "Our first(?) data table",
    "text": "Our first(?) data table\nOk, let’s try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).\n\n# create lazy table for ves (visual encounter survey) table\ndb_ves = tbl(dbcon, Id(\"survey_data\", \"ves\"))\n\nDo these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn’t look like data yet. Let’s see what we can learn from it before going to the store to collect the data.\nWhat columns the table contains:\n\n# return columns of lazy table\ncolnames(db_ves)\n\n [1] \"species_ves\"         \"count_ves\"           \"detection_location\" \n [4] \"microhab\"            \"life_stage\"          \"sex\"                \n [7] \"comments_ves\"        \"microhab_moredetail\" \"observer_ves\"       \n[10] \"visual_animal_state\" \"ves_id\"              \"survey_id\"          \n\n\nHow many total rows a table contains:\n\n# count rows\ndb_ves %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\ninteger64\n[1] 28390\n\n\nThe pull() function executes a query to return a single column or variable, synonymous with the collect() function which returns a collection of variables as a table.\nHow many rows after filtering for unknown species:\n\n# count rows with known species\ndb_ves %&gt;%\n  filter(!is.na(species_ves),\n         species_ves != \"unknown_species\") %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\ninteger64\n[1] 28232\n\n\nHow many rows corresponding to a each life stage:\n\n# count rows by life stage\ndb_ves %&gt;%\n  select(life_stage) %&gt;%\n  group_by(life_stage) %&gt;%\n  summarise(row_count = n()) %&gt;%\n  arrange(desc(row_count)) %&gt;%\n  collect()\n\n# A tibble: 8 × 2\n  life_stage row_count\n  &lt;chr&gt;        &lt;int64&gt;\n1 tadpole        10276\n2 adult           9551\n3 subadult        7162\n4 &lt;NA&gt;             764\n5 eggmass          625\n6 egg                9\n7 juvenile           2\n8 metamorph          1",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-packages-1",
    "href": "tutorial_series/02_data_discovery.html#load-packages-1",
    "title": "2. Data Discovery",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for Python DB data discovery\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#establish-database-connection-1",
    "href": "tutorial_series/02_data_discovery.html#establish-database-connection-1",
    "title": "2. Data Discovery",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# Establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-database-metadata-1",
    "href": "tutorial_series/02_data_discovery.html#load-database-metadata-1",
    "title": "2. Data Discovery",
    "section": "Load database metadata",
    "text": "Load database metadata\n\nData structure: Schemas, tables, columns and rows\nThe RIBBiTR database is organized into “schemas” (think of these as folders), which can contain any number of tables. Each table consists of columns (“variables”) and rows (“entries”).\n\n\nMetadata: Data about data\nWe keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let’s learn what tables are present in the data by loading the table metadata.\n\n\nTable Metadata\n\n# load table \"all_tables\" from schema \"public\"\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n\nSome basic database commands\nBefore we take a look at the metadata you just pulled, let’s understand the command we just ran.\n\nibis.table() - This function is used to create a “lazy” table from a data source. To specify the source, we modify the database connection dbcon. We specify the schema for the table as public (note ibis calls this “database”), as well as the table name all_tables. A “lazy” table means that the data only pulled when explicitly asked for. See execute() below.\nibis.to_pandas() - the table() function generates a “lazy” table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. “do the shopping”) we need to collect the lazy table by chaining the to_pandas() function.\n\nAlso try: Run the code above without to_pandas(), to see what an uncollected lazy table looks like.\nNow let’s take a look at the table metadata to explore what schemas and tables exist.\n\nprint(mdt)\n\n   table_schema          table_name  column_count table_description\n0      bay_area      amphib_dissect            41              None\n1      bay_area     amphib_parasite            11              None\n2      bay_area  water_quality_info            27              None\n3      bay_area                site            25              None\n4      bay_area        wetland_info            25              None\n..          ...                 ...           ...               ...\n58     kira_pep              survey            17              None\n59     kira_pep                 ves             9              None\n60     kira_pep             capture            24              None\n61     kira_pep     metadata_tables             4              None\n62     kira_pep    metadata_columns            19              None\n\n[63 rows x 4 columns]\n\n\n\n\n\nColumn metadata\nSuppose our interest is in the survey_data schema. Let’s take a closer look at the tables here by collecting metadata on table columns in this schema.\n\n# load table \"all_columns\" from schema \"public\"\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n)\n\nNotice we used the ibis.filter() command on the lazy table before calling to_pandas(). This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!\nLet’s check out the column metadata, and see what you can learn.\n\n# view dataframe\nprint(mdc)\n\n    table_schema        table_name  ... pg_description key_type\n0    survey_data              site  ...           None     None\n1    survey_data              site  ...           None     None\n2    survey_data              site  ...           None     None\n3    survey_data           capture  ...           None     None\n4    survey_data  metadata_columns  ...           None       PK\n..           ...               ...  ...            ...      ...\n335  survey_data               ves  ...           None     None\n336  survey_data               ves  ...           None     None\n337  survey_data               ves  ...           None     None\n338  survey_data               ves  ...           None     None\n339  survey_data               ves  ...           None       FK\n\n[340 rows x 19 columns]\n\n# list the columns in our column-metadata table\nmdc.columns\n\nIndex(['table_schema', 'table_name', 'column_name', 'definition', 'units',\n       'accuracy', 'scale', 'format', 'natural_key', 'reviewed', 'data_type',\n       'character_maximum_length', 'numeric_precision', 'datetime_precision',\n       'is_nullable', 'column_default', 'ordinal_position', 'pg_description',\n       'key_type'],\n      dtype='object')\n\n\nCurious about what a certain metadata column means? There’s metadata for that (metametadata?)!\n\n# view metadata on metadata columns\nmetameta = mdc[mdc['table_name'] == 'metadata_columns']\nprint(metameta)\n\n    table_schema        table_name  ... pg_description key_type\n4    survey_data  metadata_columns  ...           None       PK\n5    survey_data  metadata_columns  ...           None     None\n6    survey_data  metadata_columns  ...           None     None\n7    survey_data  metadata_columns  ...           None     None\n8    survey_data  metadata_columns  ...           None     None\n9    survey_data  metadata_columns  ...           None     None\n10   survey_data  metadata_columns  ...           None     None\n11   survey_data  metadata_columns  ...           None     None\n12   survey_data  metadata_columns  ...           None     None\n99   survey_data  metadata_columns  ...           None     None\n254  survey_data  metadata_columns  ...           None     None\n255  survey_data  metadata_columns  ...           None       PK\n256  survey_data  metadata_columns  ...           None     None\n257  survey_data  metadata_columns  ...           None     None\n258  survey_data  metadata_columns  ...           None       PK\n259  survey_data  metadata_columns  ...           None     None\n260  survey_data  metadata_columns  ...           None     None\n261  survey_data  metadata_columns  ...           None     None\n263  survey_data  metadata_columns  ...           None     None\n\n[19 rows x 19 columns]\n\n\nA few columns to point out:\n\ndefinition\nunits\ndata_type\nnatural key\n\n(more on keys later)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#our-first-data-table-1",
    "href": "tutorial_series/02_data_discovery.html#our-first-data-table-1",
    "title": "2. Data Discovery",
    "section": "Our first(?) data table",
    "text": "Our first(?) data table\nOk, let’s try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).\n\n# create lazy table for ves (visual encounter survey) table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n\nDo these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn’t look like data yet. Let’s see what we can learn from it before going to the store to collect the data.\nWhat columns the table contains:\n\n# return columns of lazy table\ndb_ves.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id']\n\n\nHow many total rows a table contains:\n\n# count rows\n(db_ves\n .count()\n .execute())\n\n28390\n\n\nThe ibis.execute() function executes a query and returns the result, regardless of the format. This is synonymous with the to_pandas() function which returns query results as a pandas dataframe where possible.\nHow many rows after filtering for unknown species:\n\n# count rows with known species\nfiltered_row_count = (\n  db_ves\n  .filter(_.species_ves.notnull() & (_.species_ves != 'unknown_species'))\n  .count()\n  .execute())\n\nprint(filtered_row_count)\n\n28232\n\n\nHow many rows corresponding to a each life stage:\n\n# count rows by life stage\nlife_stage_counts = (\n    db_ves.group_by('life_stage')\n    .aggregate(row_count=_.count())\n    .order_by(_.row_count.desc())\n    .to_pandas()\n)\n\nprint(life_stage_counts)\n\n  life_stage  row_count\n0    tadpole      10276\n1      adult       9551\n2   subadult       7162\n3       None        764\n4    eggmass        625\n5        egg          9\n6   juvenile          2\n7  metamorph          1",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "presentations/db_basics_presentation.html#presentation-overview",
    "href": "presentations/db_basics_presentation.html#presentation-overview",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Presentation Overview",
    "text": "Presentation Overview\n\nWhy database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database",
    "href": "presentations/db_basics_presentation.html#why-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database-1",
    "href": "presentations/db_basics_presentation.html#why-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_basics_presentation.html",
    "href": "presentations/db_basics_presentation.html",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "",
    "text": "Why database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#also-try",
    "href": "tutorial_series/02_data_discovery.html#also-try",
    "title": "2. Data Discovery",
    "section": "Also try",
    "text": "Also try",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database-2",
    "href": "presentations/db_basics_presentation.html#why-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-data-goals",
    "href": "presentations/db_basics_presentation.html#ribbitr-data-goals",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "RIBBiTR Data Goals",
    "text": "RIBBiTR Data Goals\nRIBBiTR Data Management & Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "presentations/db_context_presentation.html#presentation-overview",
    "href": "presentations/db_context_presentation.html#presentation-overview",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Presentation Overview",
    "text": "Presentation Overview\n\nDatabase overview\nWhy a database?\nWhat is a database?\nHow to make the most of a database?\nData in our Community\nClosing the loop"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database",
    "href": "presentations/db_context_presentation.html#why-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-1",
    "href": "presentations/db_context_presentation.html#why-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-2",
    "href": "presentations/db_context_presentation.html#why-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Data Goals\nRIBBiTR Data Plan & RIBBiTR Pre-Collab Agreement\n\nCollect/generate, standardize, & preserve project data, including: genetic, pathogen, host, environment, acoustic, model, and curricula data\nShare RIBBiTR data openly within RIBBiTR team, publishing with consent and invitations to collaborate\nAnalyze & interpret data collaboratively to communicate context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#ideal-solution",
    "href": "presentations/db_context_presentation.html#ideal-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "presentations/db_context_presentation.html#the-database-one-solution",
    "href": "presentations/db_context_presentation.html#the-database-one-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#how-it-works-kinda",
    "href": "presentations/db_context_presentation.html#how-it-works-kinda",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#results-from-survey",
    "href": "presentations/db_context_presentation.html#results-from-survey",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#closing-the-loop",
    "href": "presentations/db_context_presentation.html#closing-the-loop",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Closing the loop",
    "text": "Closing the loop\n\nProject goals, Community goals, Data goals"
  },
  {
    "objectID": "presentations/db_context_presentation.html#lets-practice",
    "href": "presentations/db_context_presentation.html#lets-practice",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "presentations/db_context_presentation.html",
    "href": "presentations/db_context_presentation.html",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "",
    "text": "Why database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-3",
    "href": "presentations/db_context_presentation.html#why-database-3",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nWant data sharing solution which are:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust, avoiding competing versions, decaying integrity, etc.\nAccessible for people with different trainings and ways of knowing\n\nDatabases are one part of such a solution"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database",
    "href": "presentations/db_context_presentation.html#why-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-1",
    "href": "presentations/db_context_presentation.html#why-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-2",
    "href": "presentations/db_context_presentation.html#why-a-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Data Goals\nRIBBiTR Data Plan & RIBBiTR Pre-Collab Agreement\n\nCollect/generate, standardize, & preserve project data, including: genetic, pathogen, host, environment, acoustic, model, and curricula data\nShare RIBBiTR data openly within RIBBiTR team, publishing with consent and invitations to collaborate\nAnalyze & interpret data collaboratively to communicate context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-3",
    "href": "presentations/db_context_presentation.html#why-a-database-3",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nWant data sharing solution which are:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust, avoiding competing versions, decaying integrity, etc.\nAccessible for people with different trainings and ways of knowing\n\nDatabases are one part of such a solution"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database",
    "href": "presentations/db_context_presentation.html#what-is-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database?",
    "text": "What is a Database?\n\nA relational database"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database-1",
    "href": "presentations/db_context_presentation.html#what-is-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database",
    "text": "What is a Database\n\nA relational database in SQL"
  },
  {
    "objectID": "presentations/db_context_presentation.html#make-the-most-of-a-database",
    "href": "presentations/db_context_presentation.html#make-the-most-of-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Make the most of a Database",
    "text": "Make the most of a Database\n\nA relational database as part of a cycle"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database-2",
    "href": "presentations/db_context_presentation.html#what-is-a-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database?",
    "text": "What is a Database?\n\nA relational database in context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#make-the-most-of-a-database-1",
    "href": "presentations/db_context_presentation.html#make-the-most-of-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Make the most of a Database",
    "text": "Make the most of a Database\n\nA relational database as part of a complex addaptive system"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community",
    "href": "presentations/db_context_presentation.html#data-in-our-community",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community-1",
    "href": "presentations/db_context_presentation.html#data-in-our-community-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community-2",
    "href": "presentations/db_context_presentation.html#data-in-our-community-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community\n\nWhat emotions come up for you when you read the word ‘data’?"
  }
]