[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RIBBiTR Data Access Center",
    "section": "",
    "text": "This is a repository for organizing and hosting data access resources within the RIBBiTR project. RIBBiTR is dedicated to advancing an understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience. As an interdisciplinary and cross-scale research project, RIBBiTR’s goals require open collaboration and communication within the team."
  },
  {
    "objectID": "db_changelog.html",
    "href": "db_changelog.html",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Dropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.15 to 16.3\n\n\n\n\n\n\n\n\nRenamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit\n\n\n\n\n\n\n\n\n\nUnique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema\n\n\n\n\n\n\n\n\nchangelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).\n\n\n\n\n\n\n\n\n\nSet “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section",
    "href": "db_changelog.html#section",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Dropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-1",
    "href": "db_changelog.html#section-1",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.15 to 16.3",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-2",
    "href": "db_changelog.html#section-2",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Renamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-3",
    "href": "db_changelog.html#section-3",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Unique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-4",
    "href": "db_changelog.html#section-4",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-5",
    "href": "db_changelog.html#section-5",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "changelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "db_changelog.html#section-6",
    "href": "db_changelog.html#section-6",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Set “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html",
    "href": "tutorial_series/04_data_workflow.html",
    "title": "4. Data Workflow",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#setup",
    "href": "tutorial_series/04_data_workflow.html#setup",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n\n# establish database connection\ndbcon = hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n# load table metadata\nmdt = tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\n# load column metadata\nmdc = tbl(dbcon, Id(\"survey_data\", \"metadata_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling",
    "href": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nPull capture table\n\n# capture table, select, filter\ndb_capture = tbl(dbcon, Id(\"survey_data\", \"capture\")) %&gt;%\n  select(species_capture,\n         body_temp_c,\n         life_stage,\n         sex,\n         capture_animal_state,\n         bd_swab_id,\n         survey_id)\n\n\n\nJoin with support tables, filter by date\n\n# create chain object\nchain_capture = tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_chain = tbl_join(dbcon, chain_capture, tbl = db_capture) %&gt;%\n  filter(date &gt;= \"2015-01-01\")\n\nJoining with bd_swab_lookup ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nJoin with Bd table\n\ndb_bd_results = tbl(dbcon, Id(\"survey_data\", \"bd_qpcr_results\")) %&gt;%\n  select(bd_swab_id,\n         detected,\n         average_target_quant)\n\ndb_capture_bd = db_capture_chain %&gt;%\n  inner_join(db_bd_results, by=\"bd_swab_id\")\n\n\n\nExplore # of samples by life stage, then filter\n\ndb_capture_bd %&gt;%\n  select(life_stage) %&gt;%\n  group_by(life_stage) %&gt;%\n  summarise(row_count = n()) %&gt;%\n  arrange(desc(row_count)) %&gt;%\n  collect()\n\n# A tibble: 13 × 2\n   life_stage              row_count\n   &lt;chr&gt;                     &lt;int64&gt;\n 1 adult                       22746\n 2 &lt;NA&gt;                         4515\n 3 juvenile                     3937\n 4 tadpole                      1666\n 5 subadult                      984\n 6 aquatic_larvae                573\n 7 metamorph                     428\n 8 larva                         417\n 9 unknown                       367\n10 terrestrial_development       330\n11 larvae                        218\n12 eggmass                         7\n13 Unknown                         2\n\ndb_capture_bd_life = db_capture_bd %&gt;%\n  filter(life_stage %in% c(\"juvenile\",\n                           \"subadult\",\n                           \"adult\"),\n         !is.na(life_stage))\n\n\n\nExplore # of samples by species, then filter\n\n(spp_summary = db_capture_bd_life%&gt;%\n  select(species_capture) %&gt;%\n  group_by(species_capture) %&gt;%\n  summarise(sample_count = n()) %&gt;%\n  arrange(desc(sample_count)) %&gt;%\n  collect())\n\n# A tibble: 131 × 2\n   species_capture           sample_count\n   &lt;chr&gt;                          &lt;int64&gt;\n 1 rana_muscosa                     13328\n 2 rana_clamitans                    2366\n 3 rana_catesbeiana                  1601\n 4 pseudacris_crucifer               1118\n 5 lithobates_sphenocephalus          965\n 6 rana_pipiens                       551\n 7 notophthalmus_viridescens          535\n 8 lithobates_chiricahuensis          453\n 9 colostethus_panamensis             429\n10 hyla_versicolor                    417\n# ℹ 121 more rows\n\n(spp_list = spp_summary %&gt;%\n  filter(sample_count &gt;= 100) %&gt;%\n  pull(species_capture))\n\n [1] \"rana_muscosa\"              \"rana_clamitans\"           \n [3] \"rana_catesbeiana\"          \"pseudacris_crucifer\"      \n [5] \"lithobates_sphenocephalus\" \"rana_pipiens\"             \n [7] \"notophthalmus_viridescens\" \"lithobates_chiricahuensis\"\n [9] \"colostethus_panamensis\"    \"hyla_versicolor\"          \n[11] \"hyla_chrysoscelis\"         \"anaxyrus_americanus\"      \n[13] \"silverstoneia_flotator\"    \"lithobates_warszewitschii\"\n[15] \"anaxyrus_fowleri\"          \"acris_blanchardi\"         \n[17] \"rhaebo_haematiticus\"       \"sachatamia_albomaculata\"  \n[19] \"ambystoma_opacum\"          \"hyla_cinerea\"             \n[21] \"lithobates_sylvaticus\"     \"pseudacris_feriarum\"      \n[23] \"smilisca_sila\"             \"plethodon_glutinosis\"     \n[25] \"ambystoma_maculatum\"       \"plethodon_cinereus\"       \n[27] \"unknown_species\"           \"desmognathus_fuscus\"      \n[29] \"lithobates_blairi\"        \n\ndb_capture_bd_life_spp = db_capture_bd_life %&gt;%\n  filter(species_capture %in% spp_list,\n         !is.na(species_capture))\n\n\n\ncollect data\n\ndata_capture_bd_query = db_capture_bd_life_spp %&gt;%\n  collect()\n\ncolnames(data_capture_bd_query)\n\n [1] \"species_capture\"      \"body_temp_c\"          \"life_stage\"          \n [4] \"sex\"                  \"capture_animal_state\" \"bd_swab_id\"          \n [7] \"survey_id\"            \"detection_type\"       \"visit_id\"            \n[10] \"date\"                 \"survey_time\"          \"site_id\"             \n[13] \"site\"                 \"region_id\"            \"region\"              \n[16] \"country_id\"           \"country_name\"         \"detected\"            \n[19] \"average_target_quant\"\n\nhead(data_capture_bd_query)\n\n# A tibble: 6 × 19\n  species_capture   body_temp_c life_stage sex   capture_animal_state bd_swab_id\n  &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;     \n1 lithobates_warsz…        18.6 juvenile   unkn… alive                150607_c01\n2 lithobates_warsz…        NA   juvenile   unkn… alive                150607_c02\n3 lithobates_warsz…        21.8 juvenile   unkn… alive                150607_j02\n4 lithobates_warsz…        21.6 juvenile   unkn… alive                150607_j03\n5 lithobates_warsz…        20   adult      fema… alive                150607_j04\n6 lithobates_warsz…        20   adult      male  alive                150607_j05\n# ℹ 13 more variables: survey_id &lt;chr&gt;, detection_type &lt;chr&gt;, visit_id &lt;chr&gt;,\n#   date &lt;date&gt;, survey_time &lt;chr&gt;, site_id &lt;chr&gt;, site &lt;chr&gt;, region_id &lt;chr&gt;,\n#   region &lt;chr&gt;, country_id &lt;chr&gt;, country_name &lt;chr&gt;, detected &lt;dbl&gt;,\n#   average_target_quant &lt;dbl&gt;\n\n\nThese data are ready to be analyzed and visualized!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#setup-1",
    "href": "tutorial_series/04_data_workflow.html#setup-1",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB Workflow\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig\nimport db_access as db\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\n# load table metadata\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load column metadata\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n  )",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling-1",
    "href": "tutorial_series/04_data_workflow.html#data-discovery-and-pulling-1",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nPull capture table\n\n# capture table, select, filter\ndb_capture = (\n  dbcon.table(database=\"survey_data\", name=\"capture\")\n  .select([\n    'species_capture',\n    'body_temp_c',\n    'life_stage',\n    'sex',\n    'capture_animal_state',\n    'bd_swab_id',\n    'survey_id'\n    ])\n  )\n\n\n\nJoin with support tables, filter to date\n\n# create chain object\nchain_capture = db.tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_chain = (\n  db.tbl_join(dbcon, chain_capture, tbl=db_capture)\n  .filter(_.date &gt;= '2015-01-01')\n  )\n\nJoining with bd_swab_lookup ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nJoin with Bd table\n\n# bd qpcr results lazy table\ndb_bd_results = (\n  dbcon.table(database=\"survey_data\", name=\"bd_qpcr_results\")\n  .select([\n    'bd_swab_id',\n    'detected',\n    'average_target_quant'\n  ])\n  )\n\n# join capture and bd tables\ndb_capture_bd = (\n  db_capture_chain\n  .inner_join(db_bd_results, db_capture_chain.bd_swab_id == db_bd_results.bd_swab_id)\n  )\n\n\n\nExplore # of samples by life stage, then filter\n\n# count by life stage\nlife_stage_counts = (\n  db_capture_bd\n  .group_by('life_stage')\n  .aggregate(row_count=_.count())\n  .order_by(_.row_count.desc())\n  .to_pandas()\n  )\n\nprint(life_stage_counts)\n\n                 life_stage  row_count\n0                     adult      22746\n1                      None       4515\n2                  juvenile       3937\n3                   tadpole       1666\n4                  subadult        984\n5            aquatic_larvae        573\n6                 metamorph        428\n7                     larva        417\n8                   unknown        367\n9   terrestrial_development        330\n10                   larvae        218\n11                  eggmass          7\n12                  Unknown          2\n\n\n# filter to desired life stages\ndb_capture_bd_life = (\n  db_capture_bd\n  .filter(_.life_stage.isin(['juvenile','subadult', 'adult']) & _.life_stage.notnull())\n  )\n\n\n\nExplore # of samples by species, then filter\n\n# count by species\nspp_summary = (\n  db_capture_bd_life\n  .group_by('species_capture')\n  .aggregate(sample_count=_.count())\n  .order_by(_.sample_count.desc())\n  .to_pandas()\n  )\nprint(spp_summary)\n\n                  species_capture  sample_count\n0                    rana_muscosa         13328\n1                  rana_clamitans          2366\n2                rana_catesbeiana          1601\n3             pseudacris_crucifer          1118\n4       lithobates_sphenocephalus           965\n..                            ...           ...\n126             silverstoneia_spp             1\n127                 diasporus_spp             1\n128        craugastor_monnichorum             1\n129               acris_crepitans             1\n130  hyalinobatrachium_talamancae             1\n\n[131 rows x 2 columns]\n\n\n# generate species list\nspp_list = spp_summary[spp_summary['sample_count'] &gt;= 100]['species_capture'].tolist()\n\n# filter to species in spp_list\ndb_capture_bd_life_spp = (\n  db_capture_bd_life\n  .filter(_.species_capture.isin(spp_list) & _.species_capture.notnull())\n  )\n\n\n\nPull data\n\n# pull data\ndata_capture_bd_query = db_capture_bd_life_spp.to_pandas()\n\n# printcolumn names\ndata_capture_bd_query.columns\n\nIndex(['species_capture', 'body_temp_c', 'life_stage', 'sex',\n       'capture_animal_state', 'bd_swab_id', 'survey_id', 'bd_swab_id_right',\n       'survey_id_right', 'visit_id', 'detection_type', 'visit_id_right',\n       'date', 'survey_time', 'site_id', 'region_id', 'site', 'site_id_right',\n       'region_id_right', 'region', 'country_id', 'country_name',\n       'country_id_right', 'detected', 'average_target_quant'],\n      dtype='object')\n\n# preview data\ndata_capture_bd_query.head()\n\n             species_capture  body_temp_c  ... detected average_target_quant\n0  lithobates_warszewitschii         18.6  ...      1.0              1007.69\n1  lithobates_warszewitschii          NaN  ...      1.0               257.43\n2  lithobates_warszewitschii         21.8  ...      1.0                56.15\n3  lithobates_warszewitschii         21.6  ...      0.0                 0.00\n4  lithobates_warszewitschii         20.0  ...      0.0                 0.00\n\n[5 rows x 25 columns]\n\n\nThese data are ready to be analyzed and visualized!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html",
    "href": "tutorial_series/01_connection_setup.html",
    "title": "1. Connection Setup",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters",
    "href": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters",
    "title": "1. Connection Setup",
    "section": "Store & access your database connection parameters",
    "text": "Store & access your database connection parameters\n\nAccess your local .Renviron file\nYour .Renviron file a local file where you can save and reference your login credentials for easy use within R and RStudio, without risking losing them or potentially sharing them on accident when you share your code. A simple way to access your .Renviron file is with the function usethis::edit_r_environ()\n\ninstall.packages(\"usethis\")\n\n# open your local .Reniron file\nusethis::edit_r_environ()\n\n\n\nSave connections parameters\nCopy the following database connection parameters to your .Renviron file, substituting your login credentials (user & password).\n\n# RIBBiTR DB credentials\nribbitr.dbname &lt;- \"ribbitr\"\nribbitr.host &lt;- \"ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com\"\nribbitr.port &lt;- \"5432\"\nribbitr.user &lt;- \"[YOUR-USERNAME-HERE]\"\nribbitr.password &lt;- \"[YOUR-PASSWORD-HERE]\"\n\nSave and close .Renviron, and restart RStudio.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#establish-database-connection",
    "href": "tutorial_series/01_connection_setup.html#establish-database-connection",
    "title": "1. Connection Setup",
    "section": "Establish database connection",
    "text": "Establish database connection\nCreate a new R project (or .qmd, .Rmd, .R etc.) file where you can follow the tutorial and establish the database connection.\n\nLoad packages\n\nRtools\nRtools is required to build R packages locally, as part of this tutorial. Check to see if Rtools is installed by running:\n\nSys.which(\"make\")\n\n           make \n\"/usr/bin/make\" \n\n\nIf this returns a path to the make function (e.g. /usr/bin/make), Rtools is installed and you can proceed to the next step. If you return an empty string \"\" you will need to download Rtools first.\n\n\nLibrarian\n“librarian” is a package and library management package in R which makes it easier to install, load, update and unload packages to meet dynamic environment needs. There are other ways to download, load, and maintain packages in R (e.g. install.packages() and library(), but we recommend librarian for its simplicity and portability.\n\n# install and load \"librarian\" R package\ninstall.packages(\"librarian\")\n\nlibrarian downloads and loads packages using the librarian::shelf function. Below are the minimal recommended packages to establish a connection to the RIBBiTR database.\n\n# minimal packages for establishing RIBBiTR DB connection\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n# librarian::shelf(RIBBiTR-BII/ribbitrrr, update_all = TRUE)\n\n\n\n\nConnect\nNow, using the ribbitrrr:hopToDB() function, let’s establish a connection!\n\n# establish database connection\ndbcon &lt;- hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n\nhopToDB() returns a database connection object (dbcon). Keep track of this, you will call it to explore and pull data later.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#begin-using-your-connection",
    "href": "tutorial_series/01_connection_setup.html#begin-using-your-connection",
    "title": "1. Connection Setup",
    "section": "Begin using your connection!",
    "text": "Begin using your connection!\nTry out your connection by loading table metadata from the database\n\nmdt &lt;- tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\nhead(mdt)\n\n# A tibble: 6 × 4\n  table_schema table_name         column_count table_description\n  &lt;chr&gt;        &lt;chr&gt;                   &lt;int64&gt; &lt;chr&gt;            \n1 bay_area     amphib_dissect               41 &lt;NA&gt;             \n2 bay_area     amphib_parasite              11 &lt;NA&gt;             \n3 bay_area     water_quality_info           27 &lt;NA&gt;             \n4 bay_area     site                         25 &lt;NA&gt;             \n5 bay_area     wetland_info                 25 &lt;NA&gt;             \n6 bay_area     bd_results                   25 &lt;NA&gt;",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#also-try",
    "href": "tutorial_series/01_connection_setup.html#also-try",
    "title": "1. Connection Setup",
    "section": "Also try",
    "text": "Also try\n\nFor those managing multiple database connections, the hopToDB() function allows you to store and fetch various sets of login credentials with a single keyword. Just substitute “ribbitr” in the .Renviron example above with your own keywords to juggle multiple logins.\nYour login credentials can also be accessed explicitly anytime using Sys.getenv(\"ribbitr.dbname\"), etc. In most cases the hopToDB() function is all you will need, however.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-1",
    "href": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-1",
    "title": "1. Connection Setup",
    "section": "Store & access your database connection parameters",
    "text": "Store & access your database connection parameters\n\nCreate a dbconfig file\nWe recommend you create a local database config (dbconfig.py) file where you can save and reference your login credentials for easy use in python, without risking losing them or potentially sharing them on accident when you share your code.\nCreate a file nammed dbconfig.py in your project working directory (or another preferred location, see “Also try” below). Copy the following to dbconfig.py:\n\n# dbconfig.py\n\nribbitr = {\n  \"database\":\"ribbitr\",\n  \"host\":\"ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com\",\n  \"port\":\"5432\",\n  \"user\":\"[YOUR-USERNAME-HERE]\",\n  \"password\":\"[YOUR-PASSWORD-HERE]\",\n}\n\nSave dbconfig.py.\nBe sure to add dbconfig.py to your local .gitignore file if you are using git/github, so you don’t accidentally publish you login credentials!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#establish-database-connection-1",
    "href": "tutorial_series/01_connection_setup.html#establish-database-connection-1",
    "title": "1. Connection Setup",
    "section": "Establish database connection",
    "text": "Establish database connection\nCreate a new .py (or .qmd, .ipynb, etc.) file where you can follow the tutorial and establish the database connection.\n\nImport packages\nThis method requires installing the ibis.postgres package to your working environment, in addition to pandas. We also import the dbconfig.py file to access your login credentials.\n\nimport ibis\nimport pandas as pd\nimport dbconfig  # import connection credentials\n\n\n\nConnect\nNow, using the ibis.postgres.connect() function, let’s establish a connection!\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\nibis.postgres.connect() returns a database connection object (dbcon). Keep track of this, you will call it to explore and pull data later.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#begin-using-your-connection-1",
    "href": "tutorial_series/01_connection_setup.html#begin-using-your-connection-1",
    "title": "1. Connection Setup",
    "section": "Begin using your connection",
    "text": "Begin using your connection\nTry out your connection by loading table metadata from the database\n\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\nmdt.head()\n\n  table_schema          table_name  column_count table_description\n0     bay_area      amphib_dissect            41              None\n1     bay_area     amphib_parasite            11              None\n2     bay_area  water_quality_info            27              None\n3     bay_area                site            25              None\n4     bay_area        wetland_info            25              None",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#also-try-1",
    "href": "tutorial_series/01_connection_setup.html#also-try-1",
    "title": "1. Connection Setup",
    "section": "Also try",
    "text": "Also try\n\nFor those managing multiple database connections, this method allows you to store and fetch various sets of login credentials with a single keyword. Just substitute “ribbitr” in the dbconfig.py file with your own keywords and call them as needed!\nIf you will be connecting to the database from different python projects, you may want to save your dbconfig.py file to a more general location. In this case, include the following lines in each of your project files:\n\n\nimport sys\nsys.path.append(\"/path/to/dbconfig/dir/\")\nimport dbconfig",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/00_tutorial_series.html",
    "href": "tutorial_series/00_tutorial_series.html",
    "title": "RIBBiTR Database Tutorial Series",
    "section": "",
    "text": "Welcome!\nThis is a tutorial series for the RIBBiTR Database. Working through the tutorials on the left, you can work through examples of how to connect to the database, explore what data is there and understand it’s structure, efficiently download the data of interest, and pass it to your workflows. This series includes examples in R and Python.\nDive into the tutorial series below:\n\nConnection Setup.\nData Discovery.\nData Pulling.\nData Workflow.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html",
    "href": "tutorial_series/03_data_pulling.html",
    "title": "3. Data Pulling",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-packages",
    "href": "tutorial_series/03_data_pulling.html#load-packages",
    "title": "3. Data Pulling",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#establish-database-connection",
    "href": "tutorial_series/03_data_pulling.html#establish-database-connection",
    "title": "3. Data Pulling",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon &lt;- hopToDB(\"ribbitr\")\n\nConnecting to database... Success!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-metadata",
    "href": "tutorial_series/03_data_pulling.html#load-metadata",
    "title": "3. Data Pulling",
    "section": "Load metadata",
    "text": "Load metadata\nWe recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).\n\n# load table \"all_tables\" from schema \"public\"\nmdt &lt;- tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\n\n# load table \"all_columns\" from schema \"public\", filtering to schema \"survey_data\"\nmdc &lt;- tbl(dbcon, Id(\"public\", \"all_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#pulling-data",
    "href": "tutorial_series/03_data_pulling.html#pulling-data",
    "title": "3. Data Pulling",
    "section": "Pulling data",
    "text": "Pulling data\nLet’s construct our first data query, building from the previous tutorial.\n\n# lazy table and collect\ndb_ves &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  collect()\n\nGreat, that was easy! But what if we don’t need all that data? Suppose we are only interested in certain columns? We can dplyr::select() for specific columns to avoid pulling unnecessary data:\n\n# lazy table, select, and collect\ndb_ves &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  collect()\n\nAnd perhaps we are only interested in adults, in which case we can also dplyr::filter() our table to desired rows before collecting:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\") %&gt;%\n  collect()\n\n# preview table\nhead(db_ves_adult)\n\n# A tibble: 6 × 5\n  species_ves            count_ves life_stage sex     survey_id                 \n  &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                     \n1 colostethus_panamensis         2 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n2 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n3 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n4 colostethus_panamensis         1 adult      &lt;NA&gt;    5ef5b5ea-dc62-4428-91d4-a…\n5 silverstoneia_flotator         1 adult      unkonwn fd70bfc9-693e-4a31-af06-5…\n6 silverstoneia_flotator         1 adult      unkonwn fd70bfc9-693e-4a31-af06-5…\n\n\nGreat! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#sql-aside",
    "href": "tutorial_series/03_data_pulling.html#sql-aside",
    "title": "3. Data Pulling",
    "section": "SQL aside",
    "text": "SQL aside\n“Wait a minute… I thought these data were encoded in SQL? Where is the SQL?” Turns out, the package dbplyr does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.\nBut if we want to see the SQL, we can! Let’s take a closer look at the lazy table for our last query (dropping the collect() statement):\n\n# lazy table only (not collected)\nves_adult &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\")\n\n# render sql from lazy table\n(ves_adult_q = sql_render(ves_adult))\n\n&lt;SQL&gt; SELECT \"species_ves\", \"count_ves\", \"life_stage\", \"sex\", \"survey_id\"\nFROM \"survey_data\".\"ves\"\nWHERE (\"life_stage\" = 'adult')\n\n\nThe dbplyr::sql_render() function converts our lazy table “shopping list” into an SQL script. If we want we can interact with this script, and even send it to the database manually using DBI::dbGetQuery()\n\n# execute SQL statement and return results\ndb_ves_adult_sql &lt;- dbGetQuery(dbcon, ves_adult_q)\n\n# preview table\nhead(db_ves_adult_sql)\n\n             species_ves count_ves life_stage     sex\n1 colostethus_panamensis         2      adult    &lt;NA&gt;\n2 colostethus_panamensis         1      adult    &lt;NA&gt;\n3 colostethus_panamensis         1      adult    &lt;NA&gt;\n4 colostethus_panamensis         1      adult    &lt;NA&gt;\n5 silverstoneia_flotator         1      adult unkonwn\n6 silverstoneia_flotator         1      adult unkonwn\n                             survey_id\n1 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n2 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n3 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n4 5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n5 fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n6 fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n\n\nOn close inspection we see that this is identical to the db_ves_adult_sql above. Now you have two ways to access the same data!\nWe will stick with the dplyr/dbplyr methods for the rest of this tutorial, but feel free to integrate this with your curiousity and/or knowledge of SQL as we go forward.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#joins",
    "href": "tutorial_series/03_data_pulling.html#joins",
    "title": "3. Data Pulling",
    "section": "Joins",
    "text": "Joins\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country?”\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nNatural Key (nk or nkey) – A meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key.\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\n\nWhen we pull a data table from the database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have of column metadata to help us keep track of this! Check out the key_type and natural_key columns for the survey table:\n\nves_metadata = mdc %&gt;%\n  select(table_name,\n         column_name,\n         key_type,\n         natural_key) %&gt;%\n  filter(table_name == \"ves\")\n\nview(ves_metadata)\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there another way?\n\n## ribbitrrr key functions\n\n# primary key for ves table\ntbl_pkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# natural key for ves table\ntbl_nkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# foreign key for ves table\ntbl_fkey(\"ves\", mdc)\n\n[1] \"survey_id\"\n\n# all unique key columns for ves table\ntbl_keys(\"ves\", mdc)\n\n[1] \"ves_id\"    \"survey_id\"\n\n\nNotice that we passed the column metadata to these functions, to help us automate this otherwise tedious task.\n\n\nJoining manually by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example:\n\n# lazy table of ves\ndb_ves = tbl(dbcon, Id(\"survey_data\", \"ves\"))\n\n# lazy table of survey\ndb_survey = tbl(dbcon, Id(\"survey_data\", \"survey\"))\n\ndb_ves_survey = db_ves %&gt;%\n  left_join(db_survey, by=\"survey_id\")\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                      \n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with by = tbl_fkey(\"ves\", mdc).\nIn order to join the VES data with country, we will have to do several, recursive joins to connect the tables of… (consults schema diagram)… survey, visit, site, region, and country! Is there another way?",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#links-chains-automated-joins",
    "href": "tutorial_series/03_data_pulling.html#links-chains-automated-joins",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some functions to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = tbl_join(dbcon, link_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                      \n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there another way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = tbl_join(dbcon, chain_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"percent_cloud_cover\"            \"weather_condition_notes\"       \n[47] \"pressure_psi_drop\"              \"relative_humidity_percent\"     \n[49] \"relative_humidity_drop_percent\" \"wind_speed_min_m_s\"            \n[51] \"wind_speed_max_m_s\"             \"air_temp_c_drop\"               \n[53] \"densiometer_d1_num_covered\"     \"d1_n\"                          \n[55] \"d1_s\"                           \"d1_e\"                          \n[57] \"d1_w\"                           \"d1_percent_cover\"              \n[59] \"densiometer_d2_num_covered\"     \"d2_n\"                          \n[61] \"d2_s\"                           \"d2_e\"                          \n[63] \"d2_w\"                           \"d2_percent_cover\"              \n[65] \"depth_of_water_from_d2_cm\"      \"percent_vegetation_cover\"      \n[67] \"vegetation_notes\"               \"secchi_depth_cm\"               \n[69] \"visit_id\"                       \"date\"                          \n[71] \"survey_time\"                    \"campaign\"                      \n[73] \"visit_status\"                   \"comments_visit\"                \n[75] \"site_id\"                        \"site\"                          \n[77] \"utm_zone\"                       \"utme\"                          \n[79] \"utmn\"                           \"area_sqr_m\"                    \n[81] \"site_code\"                      \"elevation_m\"                   \n[83] \"depth_m\"                        \"topo\"                          \n[85] \"wilderness\"                     \"site_comments\"                 \n[87] \"region_id\"                      \"region\"                        \n[89] \"country_id\"                     \"location_id\"                   \n[91] \"country_name\"                   \"iso_country_code\"              \n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in seperate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# lazy table, select, filter\ndb_ves_adult = tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\")\n\n# create chain object\nchain_ves = tbl_chain(\"ves\", mdc)\n\n# join recursively, specifying desired columns, filter, collect\ndb_ves_adult_final = tbl_join(dbcon, chain_ves, tbl = db_ves_adult, columns = c(\"country_name\")) %&gt;%\n  filter(country_name == \"brazil\")\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# pull selected, filtered, joined data\ndata_ves_adult_final = db_ves_adult_final %&gt;%\n  collect()\n\nA few differences here:\n\nWe provided our pre-selected and filtered dv_ves_adult table to the join function with tbl = db_ves_adult, rather than having it pull all the data.\nWe specified any additional columns to include with columns = c(\"country_name\"), in addition to any key columns (included by default). The result is much less columns, only those we want.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#also-try",
    "href": "tutorial_series/03_data_pulling.html#also-try",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nCheck out the SQL code for your last query with:\n\n\nsql_render(db_ves_adult_final)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-packages-1",
    "href": "tutorial_series/03_data_pulling.html#load-packages-1",
    "title": "3. Data Pulling",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for RIBBiTR DB data discovery\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#establish-database-connection-1",
    "href": "tutorial_series/03_data_pulling.html#establish-database-connection-1",
    "title": "3. Data Pulling",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#load-metadata-1",
    "href": "tutorial_series/03_data_pulling.html#load-metadata-1",
    "title": "3. Data Pulling",
    "section": "Load metadata",
    "text": "Load metadata\nWe recommend always loading the column metadata (and perhaps the table metadata) along with any data you are pulling. Not only will this give you a quick reference to identify what the data represent, but it will also allow us to automate some data pulling processes (more on that later).\n\n# load table \"all_tables\" from schema \"public\"\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load table \"all_columns\" from schema \"public\", filtering to schema \"survey_data\"\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#pulling-data-1",
    "href": "tutorial_series/03_data_pulling.html#pulling-data-1",
    "title": "3. Data Pulling",
    "section": "Pulling data",
    "text": "Pulling data\nLet’s construct our first data query, building from the previous tutorial.\n\n# lazy table and collect\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\").to_pandas()\ndb_ves.columns\n\nIndex(['species_ves', 'count_ves', 'detection_location', 'microhab',\n       'life_stage', 'sex', 'comments_ves', 'microhab_moredetail',\n       'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id'],\n      dtype='object')\n\n\nGreat, that was easy! But what if we don’t need all that data? Suppose we are only interested in certain columns? We can ibis.select() for specific columns to avoid pulling unnecessary data:\n\n# lazy table, select, and collect\ndb_ves_select = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .to_pandas()\n  )\n  \ndb_ves_select.columns\n\nIndex(['species_ves', 'count_ves', 'life_stage', 'sex', 'survey_id'], dtype='object')\n\n\nAnd perhaps we are only interested in adults, in which case we can also ibis.filter() our table to desired rows before collecting:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  .to_pandas()\n  )\n\n# preview table\ndb_ves_adult.head()\n\n              species_ves  ...                             survey_id\n0  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n1  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n2  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n3  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n4  silverstoneia_flotator  ...  fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n\n[5 rows x 5 columns]\n\n\nGreat! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#sql-aside-1",
    "href": "tutorial_series/03_data_pulling.html#sql-aside-1",
    "title": "3. Data Pulling",
    "section": "SQL aside",
    "text": "SQL aside\n“Wait a minute… I thought these data were encoded in SQL? Where is the SQL?” Turns out, the package ibis does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.\nBut if we want to see the SQL, we can! Let’s take a closer look at the lazy table for our last query (dropping the to_pandas() statement):\n\n# lazy table only (not collected)\nves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# render sql from lazy table\nves_adult.compile()\n\n'SELECT \"t0\".\"species_ves\", \"t0\".\"count_ves\", \"t0\".\"life_stage\", \"t0\".\"sex\", \"t0\".\"survey_id\" FROM \"survey_data\".\"ves\" AS \"t0\" WHERE \"t0\".\"life_stage\" = \\'adult\\''\n\n\nThe ibis.compile() function converts our lazy table “shopping list” into an SQL script. If we want we can revise with this script, and even send it to the database manually using dedicated python - SQL packages such as psycopg2 or SQLAlchemy.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#joins-1",
    "href": "tutorial_series/03_data_pulling.html#joins-1",
    "title": "3. Data Pulling",
    "section": "Joins",
    "text": "Joins\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country?”\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nNatural Key (nk or nkey) – A meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key.\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\n\nWhen we pull a data table from the database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have of column metadata to help us keep track of this! Check out the key_type and natural_key columns for the survey table:\n\nves_metadata = mdc[(mdc['table_name'] == 'ves')][['table_name', 'column_name', 'key_type', 'natural_key']]\n\nprint(ves_metadata)\n\n    table_name          column_name key_type natural_key\n98         ves               ves_id       PK        True\n124        ves  visual_animal_state     None       False\n326        ves         comments_ves     None       False\n331        ves            count_ves     None       False\n332        ves   detection_location     None       False\n333        ves           life_stage     None       False\n334        ves             microhab     None       False\n335        ves  microhab_moredetail     None       False\n336        ves         observer_ves     None       False\n337        ves                  sex     None       False\n338        ves          species_ves     None       False\n339        ves            survey_id       FK       False\n\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there not a better way?\n\n## data_access key functions\n\n# primary key for ves table\ndb.tbl_pkey(\"ves\", mdc)\n\n['ves_id']\n\n# natural key for ves table\ndb.tbl_nkey(\"ves\", mdc)\n\n['ves_id']\n\n# foreign key for ves table\ndb.tbl_fkey(\"ves\", mdc)\n\n['survey_id']\n\n# all unique key columns for ves table\ndb.tbl_keys(\"ves\", mdc)\n\n['survey_id', 'ves_id']\n\n\nNotice that we passed the column metadata to these functions, to help us automate this otherwise tedious task.\n\n\nJoining manually by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example:\n\n# ves lazy table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n# survey lazy table\ndb_survey = dbcon.table(database=\"survey_data\", name=\"survey\")\n# joined lazy table\ndb_ves_survey = db_ves.left_join(db_survey, \"survey_id\")\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with tbl_fkey(\"ves\", mdc).\nIn order to join the VES data with country, we will have to do several, recursive joins to connect the tables of… (consults schema diagram)… survey, visit, site, region, and country! Is there not a better way?",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#links-chains-automated-joins-1",
    "href": "tutorial_series/03_data_pulling.html#links-chains-automated-joins-1",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some functions to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = db.tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, link_ves, columns=\"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there not a better way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, chain_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'percent_cloud_cover', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'percent_vegetation_cover', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id', 'date', 'survey_time', 'campaign', 'visit_status', 'comments_visit', 'visit_id_right', 'site_id', 'site', 'utm_zone', 'utme', 'utmn', 'area_sqr_m', 'site_code', 'elevation_m', 'depth_m', 'topo', 'wilderness', 'site_comments', 'site_id_right', 'region_id', 'region', 'region_id_right', 'country_id', 'location_id', 'country_name', 'country_id_right', 'iso_country_code']\n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in seperate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# lazy table, select, filter\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n    ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# create chain object\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join recursively, providing selected and filtered table\ndb_ves_adult_final = (\n  db.tbl_join(dbcon, chain_ves, tbl=db_ves_adult)\n  .filter(_.country_name == 'brazil')\n  )\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n# pull selected, filtered, joined data\ndata_ves_adult_final = db_ves_adult_final.to_pandas()\n\nA few differences here:\n\nWe provided our pre-selected and filtered dv_ves_adult table to the join function with tbl = db_ves_adult, rather than having it pull all the data.\nWe specified any additional columns to include with columns = c(\"country_name), in addition to any key columns (included by default). The result is much less columns, only those we want.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#also-try-1",
    "href": "tutorial_series/03_data_pulling.html#also-try-1",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nCheck out the SQL code for your last query with:\n\n\ndb_ves_adult_final.compile()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "index.html#ribbitr-objectives",
    "href": "index.html#ribbitr-objectives",
    "title": "RIBBiTR Database Basics",
    "section": "RIBBiTR Objectives",
    "text": "RIBBiTR Objectives\nResearch goals\n\ncore questions (Q1-Q4) span\n\ntime scales\nspacial scales\nand therefore biological levels\n\n\nCommunity goals\n\nteam science, collaborative research\n\nno one person understands it all"
  },
  {
    "objectID": "index.html#ribbitr-data-sharing-plan",
    "href": "index.html#ribbitr-data-sharing-plan",
    "title": "RIBBiTR Database Basics",
    "section": "RIBBiTR Data Sharing Plan",
    "text": "RIBBiTR Data Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "index.html#ideal-solution",
    "href": "index.html#ideal-solution",
    "title": "RIBBiTR Database Basics",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "index.html#the-database-one-solution",
    "href": "index.html#the-database-one-solution",
    "title": "RIBBiTR Database Basics",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "index.html#how-it-works-kinda",
    "href": "index.html#how-it-works-kinda",
    "title": "RIBBiTR Database Basics",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "index.html#results-from-survey",
    "href": "index.html#results-from-survey",
    "title": "RIBBiTR Database Basics",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "index.html#closing-the-loop",
    "href": "index.html#closing-the-loop",
    "title": "RIBBiTR Data Access Center",
    "section": "Closing the loop",
    "text": "Closing the loop\nData access is an important component of our collaborative effort within RIBBiTR, but it does not exist in a vacuum. Sharing of data is enhanced when coupled with other forms of high-context communication, such as interpersonal dialogue, collaborative writing and messaging, code sharing, and more. Confused by the data? See something odd? Encounter a barrier? Increase the context in our communication by reaching out within the team for a conversation!\nAny questions? Contact the RIBBiTR data manager."
  },
  {
    "objectID": "index.html#lets-practice",
    "href": "index.html#lets-practice",
    "title": "RIBBiTR Database Basics",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "index.html#basic-motivations-and-mechanisms-behind-the-server",
    "href": "index.html#basic-motivations-and-mechanisms-behind-the-server",
    "title": "RIBBiTR Database Basics",
    "section": "Basic motivations and mechanisms behind the server",
    "text": "Basic motivations and mechanisms behind the server"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-objectives",
    "href": "presentations/db_basics_presentation.html#ribbitr-objectives",
    "title": "RIBBiTR Database: Collaboration with Context",
    "section": "RIBBiTR Objectives",
    "text": "RIBBiTR Objectives\nResearch goals\n\ncore questions (Q1-Q4) span\n\ntime scales\nspacial scales\nand therefore biological levels\n\n\nCommunity goals\n\nteam science, collaborative research\n\nno one person understands it all"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-data-sharing-plan",
    "href": "presentations/db_basics_presentation.html#ribbitr-data-sharing-plan",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "RIBBiTR Data Sharing Plan",
    "text": "RIBBiTR Data Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ideal-solution",
    "href": "presentations/db_basics_presentation.html#ideal-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#the-database-one-solution",
    "href": "presentations/db_basics_presentation.html#the-database-one-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#how-it-works-kinda",
    "href": "presentations/db_basics_presentation.html#how-it-works-kinda",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#results-from-survey",
    "href": "presentations/db_basics_presentation.html#results-from-survey",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#closing-the-loop",
    "href": "presentations/db_basics_presentation.html#closing-the-loop",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Closing the loop",
    "text": "Closing the loop\n\nProject goals: Data as connection & understanding (not data for data sake)\nCommunity goals: Data as participatory (communicating ad collaborating to lower barriers to access)"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#lets-practice",
    "href": "presentations/db_basics_presentation.html#lets-practice",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#basic-motivations-and-mechanisms-behind-the-server",
    "href": "presentations/db_basics_presentation.html#basic-motivations-and-mechanisms-behind-the-server",
    "title": "RIBBiTR Database: Collaboration with Context",
    "section": "Basic motivations and mechanisms behind the server",
    "text": "Basic motivations and mechanisms behind the server"
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html",
    "href": "tutorial_series/02_data_discovery.html",
    "title": "2. Data Discovery",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-packages",
    "href": "tutorial_series/02_data_discovery.html#load-packages",
    "title": "2. Data Discovery",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#establish-database-connection",
    "href": "tutorial_series/02_data_discovery.html#establish-database-connection",
    "title": "2. Data Discovery",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# establish database connection\ndbcon &lt;- hopToDB(\"ribbitr\")\n\nConnecting to database... Success!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-database-metadata",
    "href": "tutorial_series/02_data_discovery.html#load-database-metadata",
    "title": "2. Data Discovery",
    "section": "Load database metadata",
    "text": "Load database metadata\n\nData structure: Schemas, tables, columns and rows\nThe RIBBiTR database is organized into “schemas” (think of these as folders), which can contain any number of tables. Each table consists of columns (“variables”) and rows (“entries”).\n\n\nMetadata: Data about data\nWe keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let’s learn what tables are present in the data by loading the table metadata.\n\n\nTable Metadata\n\n# load table \"all_tables\" from schema \"public\"\nmdt &lt;- tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  collect()\n\n\nSome basic database commands\nBefore we take a look at the metadata you just pulled, let’s understand the command we just ran.\n\ndplyr::tbl() - This function is used to create a “lazy” table from a data source. To specify the source, we provide the database connection dbcon, as well as a pointer or “address” for the table of interest using the Id() function. A “lazy” table means that the data only pulled when explicitly asked for. See collect() below.\ndbplyr::Id() - This function is a pointer to pass hierarchical table identifiers (you can think of this as an address for a given table). In this case we use it to generate an pointer for the table “all_tables” in schema “public”.\ndplyr::collect() - the tbl() function generates a “lazy” table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. “do the shopping”) we need to pipe in the collect() function.\n\nAlso try: Run the code above without collect(), to see what a lazy table looks like.\nNow let’s take a look at the table metadata to explore what schemas and tables exist.\n\nview(mdt)\n\n\n\n\nColumn metadata\nSuppose our interest is in the survey_data schema. Let’s take a closer look at the tables here by collecting metadata on table columns in this schema.\n\n# load table \"all_columns\" from schema \"public\"\nmdc &lt;- tbl(dbcon, Id(\"public\", \"all_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\nNotice we used the dplyr::filter() command on the lazy table before running collect(). This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!\nLet’s check out the column metadata, and see what you can learn.\n\nview(mdc)\n\n# list the columns in our column-metadata table\ncolnames(mdc)\n\n [1] \"table_schema\"             \"table_name\"              \n [3] \"column_name\"              \"definition\"              \n [5] \"units\"                    \"accuracy\"                \n [7] \"scale\"                    \"format\"                  \n [9] \"reviewed\"                 \"natural_key\"             \n[11] \"primary_key\"              \"foreign_key\"             \n[13] \"unique\"                   \"is_nullable\"             \n[15] \"data_type\"                \"character_maximum_length\"\n[17] \"numeric_precision\"        \"datetime_precision\"      \n[19] \"column_default\"           \"ordinal_position\"        \n[21] \"pg_description\"           \"key_type\"                \n\n\nCurious about what a certain metadata column means? There’s metadata for that (metametadata?)!\n\n# vew metadata on metadata columns\nview(mdc %&gt;% filter(table_name == \"metadata_columns\"))\n\nA few columns to point out:\n\ndefinition\nunits\ndata_type\nnatural key\n\n(more on keys later)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#our-first-data-table",
    "href": "tutorial_series/02_data_discovery.html#our-first-data-table",
    "title": "2. Data Discovery",
    "section": "Our first(?) data table",
    "text": "Our first(?) data table\nOk, let’s try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).\n\n# create lazy table for ves (visual encounter survey) table\ndb_ves &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\"))\n\nDo these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn’t look like data yet. Let’s see what we can learn from it before going to the store to collect the data.\nWhat columns the table contains:\n\n# return columns of lazy table\ncolnames(db_ves)\n\n [1] \"species_ves\"         \"count_ves\"           \"detection_location\" \n [4] \"microhab\"            \"life_stage\"          \"sex\"                \n [7] \"comments_ves\"        \"microhab_moredetail\" \"observer_ves\"       \n[10] \"visual_animal_state\" \"ves_id\"              \"survey_id\"          \n\n\nHow many total rows a table contains:\n\n# count rows\ndb_ves %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\ninteger64\n[1] 28390\n\n\nThe pull() function executes a query to return a single column or variable, synonymous with the collect() function which returns a collection of variables as a table.\nHow many rows after filtering for unknown species:\n\n# count rows with known species\ndb_ves %&gt;%\n  filter(!is.na(species_ves),\n         species_ves != \"unknown_species\") %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\ninteger64\n[1] 28232\n\n\nHow many rows corresponding to a each life stage:\n\n# count rows by life stage\ndb_ves %&gt;%\n  select(life_stage) %&gt;%\n  group_by(life_stage) %&gt;%\n  summarise(row_count = n()) %&gt;%\n  arrange(desc(row_count)) %&gt;%\n  collect()\n\n# A tibble: 8 × 2\n  life_stage row_count\n  &lt;chr&gt;        &lt;int64&gt;\n1 tadpole        10276\n2 adult           9551\n3 subadult        7162\n4 &lt;NA&gt;             764\n5 eggmass          625\n6 egg                9\n7 juvenile           2\n8 metamorph          1",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-packages-1",
    "href": "tutorial_series/02_data_discovery.html#load-packages-1",
    "title": "2. Data Discovery",
    "section": "Load packages",
    "text": "Load packages\n\n# minimal packages for Python DB data discovery\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#establish-database-connection-1",
    "href": "tutorial_series/02_data_discovery.html#establish-database-connection-1",
    "title": "2. Data Discovery",
    "section": "Establish database connection",
    "text": "Establish database connection\n\n# Establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-database-metadata-1",
    "href": "tutorial_series/02_data_discovery.html#load-database-metadata-1",
    "title": "2. Data Discovery",
    "section": "Load database metadata",
    "text": "Load database metadata\n\nData structure: Schemas, tables, columns and rows\nThe RIBBiTR database is organized into “schemas” (think of these as folders), which can contain any number of tables. Each table consists of columns (“variables”) and rows (“entries”).\n\n\nMetadata: Data about data\nWe keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let’s learn what tables are present in the data by loading the table metadata.\n\n\nTable Metadata\n\n# load table \"all_tables\" from schema \"public\"\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n\nSome basic database commands\nBefore we take a look at the metadata you just pulled, let’s understand the command we just ran.\n\nibis.table() - This function is used to create a “lazy” table from a data source. To specify the source, we modify the database connection dbcon. We specify the schema for the table as public (note ibis calls this “database”), as well as the table name all_tables. A “lazy” table means that the data only pulled when explicitly asked for. See execute() below.\nibis.to_pandas() - the table() function generates a “lazy” table, which is basically a shopping list for the data you want to pull. In order to actually pull the data from the server to your local machine (ie. “do the shopping”) we need to collect the lazy table by chaining the to_pandas() function.\n\nAlso try: Run the code above without to_pandas(), to see what an uncollected lazy table looks like.\nNow let’s take a look at the table metadata to explore what schemas and tables exist.\n\nprint(mdt)\n\n   table_schema          table_name  column_count table_description\n0      bay_area      amphib_dissect            41              None\n1      bay_area     amphib_parasite            11              None\n2      bay_area  water_quality_info            27              None\n3      bay_area                site            25              None\n4      bay_area        wetland_info            25              None\n..          ...                 ...           ...               ...\n59  survey_data      bd_swab_lookup             1              None\n60  survey_data    metadata_columns            22              None\n61  survey_data              region             4              None\n62  survey_data              survey            57              None\n63  survey_data             country             3              None\n\n[64 rows x 4 columns]\n\n\n\n\n\nColumn metadata\nSuppose our interest is in the survey_data schema. Let’s take a closer look at the tables here by collecting metadata on table columns in this schema.\n\n# load table \"all_columns\" from schema \"public\"\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n)\n\nNotice we used the ibis.filter() command on the lazy table before calling to_pandas(). This effectively revised the shopping list before going to the store, rather than bringing home the entire store and then filtering for what you want in your kitchen. Much less (computationally) expensive!\nLet’s check out the column metadata, and see what you can learn.\n\n# view dataframe\nprint(mdc)\n\n    table_schema        table_name  ... pg_description key_type\n0    survey_data              site  ...           None     None\n1    survey_data              site  ...           None     None\n2    survey_data              site  ...           None     None\n3    survey_data           capture  ...           None     None\n4    survey_data  metadata_columns  ...           None       PK\n..           ...               ...  ...            ...      ...\n343  survey_data          location  ...           None     None\n344  survey_data  metadata_columns  ...           None     None\n345  survey_data  metadata_columns  ...           None     None\n346  survey_data  metadata_columns  ...           None     None\n347  survey_data            region  ...           None     None\n\n[348 rows x 22 columns]\n\n# list the columns in our column-metadata table\nmdc.columns\n\nIndex(['table_schema', 'table_name', 'column_name', 'definition', 'units',\n       'accuracy', 'scale', 'format', 'reviewed', 'natural_key', 'primary_key',\n       'foreign_key', 'unique', 'is_nullable', 'data_type',\n       'character_maximum_length', 'numeric_precision', 'datetime_precision',\n       'column_default', 'ordinal_position', 'pg_description', 'key_type'],\n      dtype='object')\n\n\nCurious about what a certain metadata column means? There’s metadata for that (metametadata?)!\n\n# view metadata on metadata columns\nmetameta = mdc[mdc['table_name'] == 'metadata_columns']\nprint(metameta)\n\n    table_schema        table_name  ... pg_description key_type\n4    survey_data  metadata_columns  ...           None       PK\n5    survey_data  metadata_columns  ...           None     None\n6    survey_data  metadata_columns  ...           None     None\n7    survey_data  metadata_columns  ...           None     None\n94   survey_data  metadata_columns  ...           None     None\n249  survey_data  metadata_columns  ...           None       PK\n250  survey_data  metadata_columns  ...           None       PK\n251  survey_data  metadata_columns  ...           None     None\n328  survey_data  metadata_columns  ...           None     None\n329  survey_data  metadata_columns  ...           None     None\n330  survey_data  metadata_columns  ...           None     None\n331  survey_data  metadata_columns  ...           None     None\n332  survey_data  metadata_columns  ...           None     None\n333  survey_data  metadata_columns  ...           None     None\n334  survey_data  metadata_columns  ...           None     None\n335  survey_data  metadata_columns  ...           None     None\n336  survey_data  metadata_columns  ...           None     None\n337  survey_data  metadata_columns  ...           None     None\n338  survey_data  metadata_columns  ...           None     None\n344  survey_data  metadata_columns  ...           None     None\n345  survey_data  metadata_columns  ...           None     None\n346  survey_data  metadata_columns  ...           None     None\n\n[22 rows x 22 columns]\n\n\nA few columns to point out:\n\ndefinition\nunits\ndata_type\nnatural key\n\n(more on keys later)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#our-first-data-table-1",
    "href": "tutorial_series/02_data_discovery.html#our-first-data-table-1",
    "title": "2. Data Discovery",
    "section": "Our first(?) data table",
    "text": "Our first(?) data table\nOk, let’s try to apply some of what we have learned by pulling directly from a data table. We can begin by taking a look at the visual encounter surveys (VES).\n\n# create lazy table for ves (visual encounter survey) table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n\nDo these functions look familiar? Turns out, we were pulling data all along! Of course, this is a lazy table (ie. shopping list) so it doesn’t look like data yet. Let’s see what we can learn from it before going to the store to collect the data.\nWhat columns the table contains:\n\n# return columns of lazy table\ndb_ves.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id']\n\n\nHow many total rows a table contains:\n\n# count rows\n(db_ves\n .count()\n .execute())\n\n28390\n\n\nThe ibis.execute() function executes a query and returns the result, regardless of the format. This is synonymous with the to_pandas() function which returns query results as a pandas dataframe where possible.\nHow many rows after filtering for unknown species:\n\n# count rows with known species\nfiltered_row_count = (\n  db_ves\n  .filter(_.species_ves.notnull() & (_.species_ves != 'unknown_species'))\n  .count()\n  .execute())\n\nprint(filtered_row_count)\n\n28232\n\n\nHow many rows corresponding to a each life stage:\n\n# count rows by life stage\nlife_stage_counts = (\n    db_ves.group_by('life_stage')\n    .aggregate(row_count=_.count())\n    .order_by(_.row_count.desc())\n    .to_pandas()\n)\n\nprint(life_stage_counts)\n\n  life_stage  row_count\n0    tadpole      10276\n1      adult       9551\n2   subadult       7162\n3       None        764\n4    eggmass        625\n5        egg          9\n6   juvenile          2\n7  metamorph          1",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "presentations/db_basics_presentation.html#presentation-overview",
    "href": "presentations/db_basics_presentation.html#presentation-overview",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Presentation Overview",
    "text": "Presentation Overview\n\nWhy database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database",
    "href": "presentations/db_basics_presentation.html#why-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database-1",
    "href": "presentations/db_basics_presentation.html#why-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_basics_presentation.html",
    "href": "presentations/db_basics_presentation.html",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "",
    "text": "Why database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#also-try",
    "href": "tutorial_series/02_data_discovery.html#also-try",
    "title": "2. Data Discovery",
    "section": "Also try",
    "text": "Also try",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "presentations/db_basics_presentation.html#why-database-2",
    "href": "presentations/db_basics_presentation.html#why-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?"
  },
  {
    "objectID": "presentations/db_basics_presentation.html#ribbitr-data-goals",
    "href": "presentations/db_basics_presentation.html#ribbitr-data-goals",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "RIBBiTR Data Goals",
    "text": "RIBBiTR Data Goals\nRIBBiTR Data Management & Sharing Plan\nObjective: To facilitate a deeper collective understanding of amphibian resilience to Bd\nData sharing logistics: Open data with RIBBiTR, empowered use with permission from data source"
  },
  {
    "objectID": "presentations/db_context_presentation.html#presentation-overview",
    "href": "presentations/db_context_presentation.html#presentation-overview",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Presentation Overview",
    "text": "Presentation Overview\n\nDatabase overview\n\nWhy a database?\nWhat is a database?\nHow to make the most of a database?\n\nData in our Community\nClosing the loop"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database",
    "href": "presentations/db_context_presentation.html#why-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-1",
    "href": "presentations/db_context_presentation.html#why-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-2",
    "href": "presentations/db_context_presentation.html#why-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nRIBBiTR Data Goals\nRIBBiTR Data Plan & RIBBiTR Pre-Collab Agreement\n\nCollect/generate, standardize, & preserve project data, including: genetic, pathogen, host, environment, acoustic, model, and curricula data\nShare RIBBiTR data openly within RIBBiTR team, publishing with consent and invitations to collaborate\nAnalyze & interpret data collaboratively to communicate context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#ideal-solution",
    "href": "presentations/db_context_presentation.html#ideal-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Ideal solution",
    "text": "Ideal solution\nData sharing which is:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust (avoiding multiple competing versions, decaying integrity)\nAccessible for people with different trainings and backgrounds\nDifficult to misuse"
  },
  {
    "objectID": "presentations/db_context_presentation.html#the-database-one-solution",
    "href": "presentations/db_context_presentation.html#the-database-one-solution",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "The database (one solution)",
    "text": "The database (one solution)\n(how it address the above goals)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#how-it-works-kinda",
    "href": "presentations/db_context_presentation.html#how-it-works-kinda",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "How it works (kinda)",
    "text": "How it works (kinda)\nConnecting humans to data\n(diagram of human, computer, platform {r, pytho, DBeaver}, SQL pipeline, data tables)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#results-from-survey",
    "href": "presentations/db_context_presentation.html#results-from-survey",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Results from survey",
    "text": "Results from survey\nData of interest\n(results)\nPlatforms of choice\n(results)\nData emotions\n(results)"
  },
  {
    "objectID": "presentations/db_context_presentation.html#closing-the-loop",
    "href": "presentations/db_context_presentation.html#closing-the-loop",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Closing the loop",
    "text": "Closing the loop"
  },
  {
    "objectID": "presentations/db_context_presentation.html#lets-practice",
    "href": "presentations/db_context_presentation.html#lets-practice",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Let’s practice!",
    "text": "Let’s practice!"
  },
  {
    "objectID": "presentations/db_context_presentation.html",
    "href": "presentations/db_context_presentation.html",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "",
    "text": "Why database?\nWhat is database?\nHow to make the most of database?"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-database-3",
    "href": "presentations/db_context_presentation.html#why-database-3",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why Database?",
    "text": "Why Database?\nWant data sharing solution which are:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust, avoiding competing versions, decaying integrity, etc.\nAccessible for people with different trainings and ways of knowing\n\nDatabases are one part of such a solution"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database",
    "href": "presentations/db_context_presentation.html#why-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Research Goals\nRIBBiTR Project Proposal\n\nQ1: How does the history of disturbance differ across systems?\nQ2: How have changes at different levels of biological organization shaped overall system responses?\nQ3: What are the mechanisms contributing to resilience and are they shared across systems?\nQ4: How is resilience modulated by multiple interacting stressors?\n\nSpanning, time scales, spacial scales, biological levels."
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-1",
    "href": "presentations/db_context_presentation.html#why-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Community Goals\nRIBBiTR Project Proposal & RIBBiTR Pre-Collab Agreement\n\nAdvance understanding of resilience by developing, applying, and sharing a novel framework for the study of resilience\nGrow our collaborative network\nTrain next generation of transdisciplinary scientists\nFollow and advance Open Science and Team Science"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-2",
    "href": "presentations/db_context_presentation.html#why-a-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nRIBBiTR Data Goals\nRIBBiTR Data Plan & RIBBiTR Pre-Collab Agreement\n\nCollect/generate, standardize, & preserve project data, including: genetic, pathogen, host, environment, acoustic, model, and curricula data\nShare RIBBiTR data openly within RIBBiTR team, publishing with consent and invitations to collaborate\nAnalyze & interpret data collaboratively to communicate context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#why-a-database-3",
    "href": "presentations/db_context_presentation.html#why-a-database-3",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Why a Database?",
    "text": "Why a Database?\nWant data sharing solution which are:\n\nEmpowering in exploring and testing our understanding of amphibian resilience to Bd\nRobust, avoiding competing versions, decaying integrity, etc.\nAccessible for people with different trainings and ways of knowing\n\nDatabases are one part of such a solution"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database",
    "href": "presentations/db_context_presentation.html#what-is-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database?",
    "text": "What is a Database?\n\nA relational database"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database-1",
    "href": "presentations/db_context_presentation.html#what-is-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database",
    "text": "What is a Database\n\nA relational database in SQL"
  },
  {
    "objectID": "presentations/db_context_presentation.html#make-the-most-of-a-database",
    "href": "presentations/db_context_presentation.html#make-the-most-of-a-database",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Make the most of a Database",
    "text": "Make the most of a Database\n\nA relational database as part of a cycle"
  },
  {
    "objectID": "presentations/db_context_presentation.html#what-is-a-database-2",
    "href": "presentations/db_context_presentation.html#what-is-a-database-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "What is a Database?",
    "text": "What is a Database?\n\nA relational database in context"
  },
  {
    "objectID": "presentations/db_context_presentation.html#make-the-most-of-a-database-1",
    "href": "presentations/db_context_presentation.html#make-the-most-of-a-database-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Make the most of a Database",
    "text": "Make the most of a Database\n\nA relational database as part of a complex addaptive system"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community",
    "href": "presentations/db_context_presentation.html#data-in-our-community",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community-1",
    "href": "presentations/db_context_presentation.html#data-in-our-community-1",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community"
  },
  {
    "objectID": "presentations/db_context_presentation.html#data-in-our-community-2",
    "href": "presentations/db_context_presentation.html#data-in-our-community-2",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Data in our community",
    "text": "Data in our community\n\nWhat emotions come up for you when you read the word ‘data’?"
  },
  {
    "objectID": "presentations/db_context_presentation.html#lets-practice-together",
    "href": "presentations/db_context_presentation.html#lets-practice-together",
    "title": "RIBBiTR Database: Collaboration and Context",
    "section": "Let’s practice together!",
    "text": "Let’s practice together!"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "RIBBiTR Data Access Center",
    "section": "Getting started",
    "text": "Getting started\nTo begin, take a look at the resources provided in the menu to the left!"
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-2",
    "href": "tutorial_series/01_connection_setup.html#store-access-your-database-connection-parameters-2",
    "title": "1. Connection Setup",
    "section": "Store & access your database connection parameters",
    "text": "Store & access your database connection parameters\nOpen DBeaver.\n\n\n\nRight-click in the Database Navigator” panel and select Create -&gt; Connection",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#set-up-and-test-your-connection.",
    "href": "tutorial_series/01_connection_setup.html#set-up-and-test-your-connection.",
    "title": "1. Connection Setup",
    "section": "Set up and test your connection.",
    "text": "Set up and test your connection.\nOpen DBeaver.\n  Click Test Connection in the bottom left to make sure your credentials work. Then click OK.\nThat’s it, you are ready to start using your connection!",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#load-database-metadata-2",
    "href": "tutorial_series/02_data_discovery.html#load-database-metadata-2",
    "title": "2. Data Discovery",
    "section": "Load database metadata",
    "text": "Load database metadata\n\nData structure: Schemas, tables, columns and rows\nThe RIBBiTR database is organized into “schemas” (think of these as folders), which can contain any number of tables. Each table consists of columns (“variables”) and rows (“entries”). You can explore this structure through the dropdown menu in the “Database Navigator” panel on the left.\n\n\n\nNavigate to Databases -&gt; ribbitr -&gt; Schemas\n\n\n\n\nMetadata: Data about data\nWe keep track of information regarding what tables, and columns exist in the database, and what information they are designed to describe, using table and column metadata. To begin our process of data discovery, let’s learn what tables are present in the data by loading the table metadata.\n\n\nTable Metadata\n See what you can learn about the tables in the database form the table metadata.\n\n\nColumn metadata\nSuppose our interest is in the survey_data schema. Let’s take a closer look at the tables here by collecting metadata on table columns in this schema.\n Click on the dropdown arrow next to table_schema, click on Order by table_schema ASC. Repeat for the table_name and column_name columns.\nScroll down until you see rows with table_schema = survey_data. Explore a table of interest t see what you can learn.\nCurious about what a certain metadata column means? There’s metadata for that (metametadata?)! Scroll down to table_name = metadata_columns to learn what the different columns in the current table mean.\nA few columns to point out:\n\ndefinition\nunits\ndata_type\nnatural key\n\n(more on keys later)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#schema-sructure",
    "href": "tutorial_series/02_data_discovery.html#schema-sructure",
    "title": "2. Data Discovery",
    "section": "Schema sructure",
    "text": "Schema sructure\n\n\n\nNavigate to Databases -&gt; ribbitr -&gt; Schemas -&gt; survey_data. Right-click and select View Schema. Select the ER Diagram tab.\n\n\nThis shows a diagram of the different tables within the surevy_data schema, as well as their columns and any relationships between tables. This is a useful visual reference for later, when we begin joining tables.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#our-first-data-table-2",
    "href": "tutorial_series/02_data_discovery.html#our-first-data-table-2",
    "title": "2. Data Discovery",
    "section": "Our first data table",
    "text": "Our first data table\nTo begin looking at data, let’s navigate to the visual encounter surveys (VES).\n This is your first look at field data within the database! From here you can explore organizing the data by columns, as well as exporting the table to a .csv.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#disconnect",
    "href": "tutorial_series/03_data_pulling.html#disconnect",
    "title": "3. Data Pulling",
    "section": "Disconnect",
    "text": "Disconnect\n\ndbDisconnect(dbcon)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/03_data_pulling.html#disconnect-1",
    "href": "tutorial_series/03_data_pulling.html#disconnect-1",
    "title": "3. Data Pulling",
    "section": "Disconnect",
    "text": "Disconnect\n\n# close connection\ndbcon.disconnect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "3. Data Pulling"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#disconnect",
    "href": "tutorial_series/01_connection_setup.html#disconnect",
    "title": "1. Connection Setup",
    "section": "Disconnect",
    "text": "Disconnect\nIt is good practice to close your database connection, to let the server know it can stop listening for you (otherwise it will continue to use server resources in anticipation of your). You can think of this as saying goodbye at the end of a phone call.\n\n# disconnect from database\ndbDisconnect(dbcon)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#disconnect-1",
    "href": "tutorial_series/01_connection_setup.html#disconnect-1",
    "title": "1. Connection Setup",
    "section": "Disconnect",
    "text": "Disconnect\nIt is good practice to close your database connection, to let the server know it can stop listening for you (otherwise it will continue to use server resources in anticipation of your). You can think of this as saying goodbye at the end of a phone call.\n\n# close the connection\ndbcon.disconnect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/01_connection_setup.html#disconnect-2",
    "href": "tutorial_series/01_connection_setup.html#disconnect-2",
    "title": "1. Connection Setup",
    "section": "Disconnect",
    "text": "Disconnect\nWhen you are done, click the red Disconnect button in top left corner to terminate your connection.\nIt is good practice to close your database connection, to let the server know it can stop listening for you (otherwise it will continue to use server resources in anticipation of your). You can think of this as saying goodbye at the end of a phone call.",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "1. Connection Setup"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#disconnect",
    "href": "tutorial_series/02_data_discovery.html#disconnect",
    "title": "2. Data Discovery",
    "section": "Disconnect",
    "text": "Disconnect\nReinforcing best practice by disconnecting from the server.\n\ndbDisconnect(dbcon)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/02_data_discovery.html#disconnect-1",
    "href": "tutorial_series/02_data_discovery.html#disconnect-1",
    "title": "2. Data Discovery",
    "section": "Disconnect",
    "text": "Disconnect\nReinforcing best practice by disconnecting from the server.\n\n# close connection\ndbcon.disconnect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "2. Data Discovery"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#disconnect",
    "href": "tutorial_series/04_data_workflow.html#disconnect",
    "title": "4. Data Workflow",
    "section": "Disconnect",
    "text": "Disconnect\n\ndbDisconnect(dbcon)",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_data_workflow.html#disconnect-1",
    "href": "tutorial_series/04_data_workflow.html#disconnect-1",
    "title": "4. Data Workflow",
    "section": "Disconnect",
    "text": "Disconnect\n\n# close connection\ndbcon.disconnect()",
    "crumbs": [
      "RIBBiTR Database Tutorial Series",
      "4. Data Workflow"
    ]
  },
  {
    "objectID": "tutorial_series/04_table_joins.html",
    "href": "tutorial_series/04_table_joins.html",
    "title": "3. Data Pulling",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#setup",
    "href": "tutorial_series/04_table_joins.html#setup",
    "title": "3. Data Pulling",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n\n# establish database connection\ndbcon &lt;- hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n# load table metadata\nmdt &lt;- tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\n# load column metadata\nmdc &lt;- tbl(dbcon, Id(\"survey_data\", \"metadata_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#the-need-for-joins",
    "href": "tutorial_series/04_table_joins.html#the-need-for-joins",
    "title": "3. Data Pulling",
    "section": "The need for joins",
    "text": "The need for joins\nLet’s begin with the query we constructed in our last tutorial:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\")) %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id) %&gt;%\n  filter(life_stage == \"adult\") %&gt;%\n  collect()\n\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country? or what if I want data from 2022 only?”\nSo far we have practiced pulling data from individual tables, but often we find that the data we need is stored accross multiple tables? We have to “join” these tables to bring the data we need together."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#table-join-basics",
    "href": "tutorial_series/04_table_joins.html#table-join-basics",
    "title": "3. Data Pulling",
    "section": "Table join basics",
    "text": "Table join basics\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\nNatural Key (nk or nkey) – a meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key. Maintained in metadata as important columns, not always tracked in database relationships.\n\nWhen we pull a data table from a database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have column metadata to help us keep track of this! Check out the primary_key and foreign_key columns for the survey table:\n\nves_metadata &lt;- mdc %&gt;%\n  select(table_name,\n         column_name,\n         primary_key,\n         foreign_key,\n         natural_key) %&gt;%\n  filter(table_name == \"ves\")\n\nview(ves_metadata)\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there another way?\n\n## ribbitrrr key functions\n\n# primary key column for ves table\ntbl_pkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# foreign key column(s) for ves table\ntbl_fkey(\"ves\", mdc)\n\n[1] \"survey_id\"\n\n# natural key column(s) for ves table\ntbl_nkey(\"ves\", mdc)\n\n[1] \"ves_id\"\n\n# all unique key columns for ves table\ntbl_keys(\"ves\", mdc)\n\n[1] \"ves_id\"    \"survey_id\"\n\n\nNotice that we passed the column metadata to these functions, to help us automate key lookups."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#joining-tables-by-keys",
    "href": "tutorial_series/04_table_joins.html#joining-tables-by-keys",
    "title": "3. Data Pulling",
    "section": "Joining tables by keys",
    "text": "Joining tables by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example, let’s join the ves and survey tables along the ves foreign key of survey_id:\n\n# lazy table of ves\ndb_ves &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\"))\n\n# lazy table of survey\ndb_survey &lt;- tbl(dbcon, Id(\"survey_data\", \"survey\"))\n\nleft_ves_survey &lt;- db_ves %&gt;%\n  left_join(db_survey, by=\"survey_id\")\n\n# check columns\ncolnames(left_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"weather_condition_notes\"        \"pressure_psi_drop\"             \n[47] \"relative_humidity_percent\"      \"relative_humidity_drop_percent\"\n[49] \"wind_speed_min_m_s\"             \"wind_speed_max_m_s\"            \n[51] \"air_temp_c_drop\"                \"densiometer_d1_num_covered\"    \n[53] \"d1_n\"                           \"d1_s\"                          \n[55] \"d1_e\"                           \"d1_w\"                          \n[57] \"d1_percent_cover\"               \"densiometer_d2_num_covered\"    \n[59] \"d2_n\"                           \"d2_s\"                          \n[61] \"d2_e\"                           \"d2_w\"                          \n[63] \"d2_percent_cover\"               \"depth_of_water_from_d2_cm\"     \n[65] \"vegetation_cover_percent\"       \"vegetation_notes\"              \n[67] \"secchi_depth_cm\"                \"visit_id\"                      \n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with by = tbl_fkey(\"ves\", mdc).\nWhen we join two tables along a key column, we are asking to align rows in the two tables where this key column is equal. In this example, we are returning a wider table where rows from the VES table are aligned with rows from the survey table where survey_id is equal in both.\nLet’s take a closer look at the number of rows in our tables to understand better what is going on.\n\nves_rows &lt;- db_ves %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\n(ves_rows)\n\ninteger64\n[1] 28390\n\nsurvey_rows &lt;- db_survey %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\n(survey_rows)\n\ninteger64\n[1] 23111\n\nleft_ves_survey_rows &lt;- left_ves_survey %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\n(left_ves_survey_rows)\n\ninteger64\n[1] 28390\n\n\nNotice that in this case, the number of rows in the joined db_ves_survey table is equal to the number ofrows in the db_ves table. This has to do with the type of join we used to join these tables, in this case a “left join”.\n\nTypes of joins\nJoins unite rows from two tables along shared key column values, allowing unshared columns between the two tables to be brought into a single table (as we see above in db_ves_survey). However,here are different joins to handle key columns vales that are not shared between tables (ie. are unique to one table). The following graphic helps illustrate the different join types:\n\n\n\nTypes of dplyr joins. Graphic by Software Carpentry used under Creative Commons 4.0\n\n\nThe graphic above describes the treatment of key column values which are either unique to one table or shared in both, using Venn diagram. In our db_ves_survey example, our use of left_join() essentially says “return everything found in the 1st (”left”) table (ie. db_ves) regardless of if it has a match in the 2nd (“right”) table (ie. db_survey). This explains why the output row numbers are identical to those in db_ves.\nUsing this diagram, let’s try a different scenario. Let’s instead use a full_join() to return all rows from each table, regardless of whether they have a matching survey_id in the other.\n\nfull_ves_survey &lt;- db_ves %&gt;%\n  full_join(db_survey, by=\"survey_id\")\n\nfull_ves_survey_rows &lt;- full_ves_survey %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\n(full_ves_survey_rows)\n\ninteger64\n[1] 41689\n\n\nIn this case we see a greater number of rows returned than are present in either table (though less than the sum of rows in each table). This tells us that some rows did align between tables, while others did not. Specifically, full_ves_survey_rows returns all VES observations (which have a corresponding survey_id in the survey table by design, see note below) as well as all other surveys in the survey table, even those that do not correspond to a VES survey. We can see this quickly by comparing the distinct detection_type’s seen in each joined table:\n\nleft_ves_survey %&gt;%\n  select(detection_type) %&gt;%\n  distinct()\n\n# Source:   SQL [1 x 1]\n# Database: postgres  [cob_reads@ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com:5432/ribbitr]\n  detection_type\n  &lt;chr&gt;         \n1 visual        \n\nfull_ves_survey %&gt;%\n  select(detection_type) %&gt;%\n  distinct()\n\n# Source:   SQL [4 x 1]\n# Database: postgres  [cob_reads@ribbitr.c6p56tuocn5n.us-west-1.rds.amazonaws.com:5432/ribbitr]\n  detection_type\n  &lt;chr&gt;         \n1 aural         \n2 other         \n3 visual        \n4 capture       \n\n\nNote: In this case, the survey table is intentionally designed such that all survey_id’s in the ves table. To confirm this we can try:\n\nanti_ves_survey &lt;- db_ves %&gt;%\n  anti_join(db_survey, by=\"survey_id\")\n\nanti_ves_survey_rows &lt;- anti_ves_survey %&gt;%\n  summarise(row_count = n()) %&gt;%\n  pull(row_count)\n\n(anti_ves_survey_rows)\n\ninteger64\n[1] 0\n\n\nThere are 0 rows in this anti_join. This means that in our Venn diagram, the left lobe is essentially empty or nonexistent. The outcome is that in this case, left_join() and inner_join() are equivalent, as are full_join() and right_join(). This is not true in general, however.\n\nRelationship cardinality\nForeign keys which point to primary keys are an example of a many:1 (“many-to-one”) relationship This is to say that a foreign key column can hold any number of duplicate values, while a primary key value can show up a maximum of once. Many:1 relationships are most common in our database. Occasionally you may see a 1:1 relationship or a many:many relationship.\n\n\n\nDeciding how to join tables\nYou have the liberty to decide which join you want, so how do you make that choice? Do you want - all the data from one table (left_join() or right_join()) - all the data or from both tables (full_join) - only data that is shared between the two (inner_join())\nTo join tables properly along shared and key columns, it is important to understand the structure of the database. This is where consulting the column metadata (mdc) or schema diagram comes in handy.\nLooking at the schema diagram, we see that to join the VES data with country, we will have to recursively join the survey, visit, site, region, and country tables."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#links-chains-automated-joins",
    "href": "tutorial_series/04_table_joins.html#links-chains-automated-joins",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some additional functions in the ribbitrrr package to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = db.tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, link_ves, join=\"left\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nGreat, similar results to our previous manual join of ves and survey, but do I need to do this recursively to get to the country table? Is there another way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, chain_ves, join=\"left\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id', 'date', 'survey_time', 'campaign', 'visit_status', 'comments_visit', 'visit_id_right', 'site_id', 'site', 'utm_zone', 'utme', 'utmn', 'area_sqr_m', 'site_code', 'elevation_m', 'depth_m', 'topo', 'wilderness', 'site_comments', 'site_id_right', 'region_id', 'region', 'region_id_right', 'country_id', 'location_id', 'country_name', 'country_id_right', 'iso_country_code']\n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in separate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# join recursively, specifying desired columns, filter, collect\ndb_ves_filtered = (\n  db.tbl_join(dbcon, chain_ves, join=\"left\")\n  .select(\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id',\n    'site',\n    'date',\n    'country_name')\n    .filter([\n      (_.life_stage == 'adult') &\n      (_.date.year() == 2022) &\n      (_.country_name == 'brazil')\n      ])\n)\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\ndata_ves_filtered = db_ves_filtered.to_pandas()\n\nNote that you can specify the recursive join type using tbl_join(..., join = \"full\") where valid options are “left”, “full”, “inner”, and “right”"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#disconnect",
    "href": "tutorial_series/04_table_joins.html#disconnect",
    "title": "3. Data Pulling",
    "section": "Disconnect",
    "text": "Disconnect\n\ndbDisconnect(dbcon)"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#also-try",
    "href": "tutorial_series/04_table_joins.html#also-try",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nRun ?tbl_chain and ?tbl_join to Learn more about other possible parameters to pass to these functions\nCheck out the SQL code for your last query with:\n\n\nsql_render(db_ves_adult_final)"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#load-packages",
    "href": "tutorial_series/04_table_joins.html#load-packages",
    "title": "3. Data Pulling",
    "section": "Load packages",
    "text": "Load packages\nFor the second half of this tutorial, you will need to download the db_access.py script and save it alongside your dbconfig.py file. This script will provide us with some useful functions for recursively joining tables"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#setup-1",
    "href": "tutorial_series/04_table_joins.html#setup-1",
    "title": "3. Data Pulling",
    "section": "Setup",
    "text": "Setup\nThese setup steps will all be familiar to you by now.\n\n# minimal packages for RIBBiTR DB Workflow\nimport pandas as pd\nimport ibis\nfrom ibis import _\nimport dbconfig\nimport db_access as db\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\n# load table metadata\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load column metadata\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n  )"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#pulling-data",
    "href": "tutorial_series/04_table_joins.html#pulling-data",
    "title": "3. Data Pulling",
    "section": "Pulling data",
    "text": "Pulling data\nLet’s construct our first data query, building from the previous tutorial.\n\n# lazy table and collect\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\").to_pandas()\ndb_ves.columns\n\nIndex(['species_ves', 'count_ves', 'detection_location', 'microhab',\n       'life_stage', 'sex', 'comments_ves', 'microhab_moredetail',\n       'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id'],\n      dtype='object')\n\n\nGreat, that was easy! But what if we don’t need all that data? Suppose we are only interested in certain columns? We can ibis.select() for specific columns to avoid pulling unnecessary data:\n\n# lazy table, select, and collect\ndb_ves_select = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .to_pandas()\n  )\n  \ndb_ves_select.columns\n\nIndex(['species_ves', 'count_ves', 'life_stage', 'sex', 'survey_id'], dtype='object')\n\n\nAnd perhaps we are only interested in adults, in which case we can also ibis.filter() our table to desired rows before collecting:\n\n# lazy table select, filter, and collect all in one\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  .to_pandas()\n  )\n\n# preview table\ndb_ves_adult.head()\n\n              species_ves  ...                             survey_id\n0  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n1  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n2  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n3  colostethus_panamensis  ...  5ef5b5ea-dc62-4428-91d4-a55ba965ed47\n4  silverstoneia_flotator  ...  fd70bfc9-693e-4a31-af06-58b0c2b3ca1c\n\n[5 rows x 5 columns]\n\n\nGreat! The above script is an example of how we can efficiently pull the data of interest without having to pull excess data."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#sql-aside",
    "href": "tutorial_series/04_table_joins.html#sql-aside",
    "title": "3. Data Pulling",
    "section": "SQL aside",
    "text": "SQL aside\n“Wait a minute… I thought these data were encoded in SQL? Where is the SQL?” Turns out, the package ibis does all the heavy lifting for us, to convert our lazy table shopping lists into SQL code which is then run on the back end without us ever having to touch it.\nBut if we want to see the SQL, we can! Let’s take a closer look at the lazy table for our last query (dropping the to_pandas() statement):\n\n# lazy table only (not collected)\nves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# render sql from lazy table\nves_adult.compile()\n\n'SELECT \"t0\".\"species_ves\", \"t0\".\"count_ves\", \"t0\".\"life_stage\", \"t0\".\"sex\", \"t0\".\"survey_id\" FROM \"survey_data\".\"ves\" AS \"t0\" WHERE \"t0\".\"life_stage\" = \\'adult\\''\n\n\nThe ibis.compile() function converts our lazy table “shopping list” into an SQL script. If we want we can revise with this script, and even send it to the database manually using dedicated python - SQL packages such as psycopg2 or SQLAlchemy."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#joins",
    "href": "tutorial_series/04_table_joins.html#joins",
    "title": "3. Data Pulling",
    "section": "Joins",
    "text": "Joins\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country?”\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nNatural Key (nk or nkey) – A meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key.\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\n\nWhen we pull a data table from the database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have of column metadata to help us keep track of this! Check out the key_type and natural_key columns for the survey table:\n\nves_metadata = mdc[(mdc['table_name'] == 'ves')][['table_name', 'column_name', 'key_type', 'natural_key']]\n\nprint(ves_metadata)\n\n    table_name          column_name key_type  natural_key\n93         ves               ves_id       PK         True\n119        ves  visual_animal_state     None        False\n314        ves         comments_ves     None        False\n319        ves            count_ves     None        False\n320        ves   detection_location     None        False\n321        ves           life_stage     None        False\n322        ves             microhab     None        False\n323        ves  microhab_moredetail     None        False\n324        ves         observer_ves     None        False\n325        ves                  sex     None        False\n326        ves          species_ves     None        False\n327        ves            survey_id       FK        False\n\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there not a better way?\n\n## data_access key functions\n\n# primary key for ves table\ndb.tbl_pkey(\"ves\", mdc)\n\n['ves_id']\n\n# natural key for ves table\ndb.tbl_nkey(\"ves\", mdc)\n\n['ves_id']\n\n# foreign key for ves table\ndb.tbl_fkey(\"ves\", mdc)\n\n['survey_id']\n\n# all unique key columns for ves table\ndb.tbl_keys(\"ves\", mdc)\n\n['survey_id', 'ves_id']\n\n\nNotice that we passed the column metadata to these functions, to help us automate this otherwise tedious task.\n\n\nJoining manually by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example:\n\n# ves lazy table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n# survey lazy table\ndb_survey = dbcon.table(database=\"survey_data\", name=\"survey\")\n# joined lazy table\ndb_ves_survey = db_ves.left_join(db_survey, \"survey_id\")\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with tbl_fkey(\"ves\", mdc).\nIn order to join the VES data with country, we will have to do several, recursive joins to connect the tables of… (consults schema diagram)… survey, visit, site, region, and country! Is there not a better way?"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#links-chains-automated-joins-1",
    "href": "tutorial_series/04_table_joins.html#links-chains-automated-joins-1",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Automated Joins",
    "text": "Links, Chains, Automated Joins\nWe developed some functions to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves = db.tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, link_ves, columns=\"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there not a better way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey = db.tbl_join(dbcon, chain_ves, columns = \"all\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ndb_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id', 'date', 'survey_time', 'campaign', 'visit_status', 'comments_visit', 'visit_id_right', 'site_id', 'site', 'utm_zone', 'utme', 'utmn', 'area_sqr_m', 'site_code', 'elevation_m', 'depth_m', 'topo', 'wilderness', 'site_comments', 'site_id_right', 'region_id', 'region', 'region_id_right', 'country_id', 'location_id', 'country_name', 'country_id_right', 'iso_country_code']\n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in seperate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# lazy table, select, filter\ndb_ves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n    ])\n  .filter(_.life_stage == 'adult')\n  )\n\n# create chain object\nchain_ves = db.tbl_chain(\"ves\", mdc)\n\n# join recursively, providing selected and filtered table\ndb_ves_adult_final = (\n  db.tbl_join(dbcon, chain_ves, tbl=db_ves_adult)\n  .filter(_.country_name == 'brazil')\n  )\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n# pull selected, filtered, joined data\ndata_ves_adult_final = db_ves_adult_final.to_pandas()\n\nA few differences here:\n\nWe provided our pre-selected and filtered dv_ves_adult table to the join function with tbl = db_ves_adult, rather than having it pull all the data.\nWe specified any additional columns to include with columns = c(\"country_name), in addition to any key columns (included by default). The result is much less columns, only those we want."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#disconnect-1",
    "href": "tutorial_series/04_table_joins.html#disconnect-1",
    "title": "3. Data Pulling",
    "section": "Disconnect",
    "text": "Disconnect\n\ndbcon.disconnect()"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#also-try-1",
    "href": "tutorial_series/04_table_joins.html#also-try-1",
    "title": "3. Data Pulling",
    "section": "Also try:",
    "text": "Also try:\n\nCheck out the SQL code for your last query with:\n\n\ndb_ves_filtered.compile()"
  },
  {
    "objectID": "resources/db_changelog.html",
    "href": "resources/db_changelog.html",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "survey.start_time established as natural key (along with survey.visit_id and survey.detection_type)\n\n\n\n\n\n\n\n\nqpcr_bd_results -&gt; bd_qpcr_results\nlocation -&gt; country\n\nlocation.location -&gt; country.country_name\n\nsurvey\n\nsurvey.percent_vegetation_cover -&gt; survey.vegetation_cover_percent\nsurvey.percent_cloud_cover coalesced with survey.cloud_cover_percent and dropped\n\n\n\n\n\n\ncountry.iso_country_code\n\n\n\n\n\n\n\n\nDropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.15 to 16.3\n\n\n\n\n\n\n\n\nRenamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit\n\n\n\n\n\n\n\n\n\nUnique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns\n\n\n\n\n\n\n\n\n\nUpgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema\n\n\n\n\n\n\n\n\nchangelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).\n\n\n\n\n\n\n\n\n\nSet “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section",
    "href": "resources/db_changelog.html#section",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "survey.start_time established as natural key (along with survey.visit_id and survey.detection_type)",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-1",
    "href": "resources/db_changelog.html#section-1",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "qpcr_bd_results -&gt; bd_qpcr_results\nlocation -&gt; country\n\nlocation.location -&gt; country.country_name\n\nsurvey\n\nsurvey.percent_vegetation_cover -&gt; survey.vegetation_cover_percent\nsurvey.percent_cloud_cover coalesced with survey.cloud_cover_percent and dropped\n\n\n\n\n\n\ncountry.iso_country_code",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-2",
    "href": "resources/db_changelog.html#section-2",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Dropped visits with NULL date\nCreated not null constraint on\n\nvisit.date\nvisit.site_id\nsite.site\nlocation.location",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-3",
    "href": "resources/db_changelog.html#section-3",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.15 to 16.3",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-4",
    "href": "resources/db_changelog.html#section-4",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Renamed possibly ambiguous columns in multiple tables in survey_data schema\n\nsurvey.observers -&gt; survey.observers_survey\nsurvey.comments -&gt; survey.comments_survey\naural.observer -&gt; aural.observer_aural\naural.count -&gt; aural.count_aural\naural.comments -&gt; aural.comments_aural\ncapture.observer -&gt; capture.observer_capture\ncapture.comments -&gt; capture.comments_capture\nves.observer -&gt; ves.observer_ves\nves.count -&gt; ves.count_ves\nves.comments -&gt; ves.comments_ves\nvisit.comments -&gt; visit.comments_visit",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-5",
    "href": "resources/db_changelog.html#section-5",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Unique constraints for natural keys on survey_data schema for the following columns:\n\nlocation table: location\nregion table: region\nsite table: site\nsurvey table: visit_id, detection_type\nvisit table: site_id, date, survey_time\n\npublic schema views of all database metadata:\n\npublic.all_tables\npublic.all_columns",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-6",
    "href": "resources/db_changelog.html#section-6",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Upgraded postgreSQL engine from version 13.9 to 13.15\nUpdated certificate authority to rds-ca-rsa2048-g1. If connecting to server using SSL, download certificate bundle here\nSet “track_commit_timestamp = on” for easier troubleshooting or and rollbacks\nCreated public schema\nEnabled extensions postgis and uuid-ossp on public schema",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-7",
    "href": "resources/db_changelog.html#section-7",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "changelog to track and share database changes\nmetadata tables: Metadata to help with documentation, communication of table and column purposes, and automation of data management. Each schema in RIBBiTR_DB now has two metadata tables:\n\nmetadata_tables: provides lookup details on each table in the schema. All columns are derived from postgres information_schema.\nmetadata_columns: provides lookup details on each column in each table in the schema. Some columns are derived from postgres information_schema, others are defined manually (see metadata_columns to see which specific metadata columns are user-defined).",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "resources/db_changelog.html#section-8",
    "href": "resources/db_changelog.html#section-8",
    "title": "ribbitr_db Changelog",
    "section": "",
    "text": "Set “log_statement = mod” to log all database modifications for accountability and troubleshooting",
    "crumbs": [
      "Database updates",
      "Database Changelog"
    ]
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html",
    "href": "tutorial_series/05_data_workflow.html",
    "title": "4. Data Workflow",
    "section": "",
    "text": "This tutorial is available as a .qmd on Github."
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#setup",
    "href": "tutorial_series/05_data_workflow.html#setup",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\n\n# minimal packages for RIBBiTR DB data discovery\nlibrarian::shelf(tidyverse, dbplyr, RPostgres, DBI, RIBBiTR-BII/ribbitrrr)\n\n# establish database connection\ndbcon &lt;- hopToDB(\"ribbitr\")\n\nConnecting to database... Success!\n\n# load table metadata\nmdt &lt;- tbl(dbcon, Id(\"public\", \"all_tables\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()\n\n# load column metadata\nmdc &lt;- tbl(dbcon, Id(\"survey_data\", \"metadata_columns\")) %&gt;%\n  filter(table_schema == \"survey_data\") %&gt;%\n  collect()"
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#data-discovery-and-pulling",
    "href": "tutorial_series/05_data_workflow.html#data-discovery-and-pulling",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nJoin with support tables, filter by date\n\n# pointers for all tables\ndb_capture &lt;- tbl(dbcon, Id(\"survey_data\", \"capture\"))\ndb_survey &lt;- tbl(dbcon, Id(\"survey_data\", \"survey\"))\ndb_visit &lt;- tbl(dbcon, Id(\"survey_data\", \"visit\"))\ndb_site &lt;- tbl(dbcon, Id(\"survey_data\", \"site\"))\ndb_region &lt;- tbl(dbcon, Id(\"survey_data\", \"region\"))\ndb_country &lt;- tbl(dbcon, Id(\"survey_data\", \"country\"))\n\n# join recursively\ndb_capture_sup &lt;- db_capture %&gt;%\n  left_join(db_survey, by = \"survey_id\") %&gt;%\n  left_join(db_visit, by = \"visit_id\") %&gt;%\n  left_join(db_site, by = \"site_id\") %&gt;%\n  left_join(db_region, by = \"region_id\") %&gt;%\n  left_join(db_country, by = \"country_id\")\n\nOr alternatively:\n\n# create chain object\nchain_capture &lt;- tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_sup &lt;- tbl_left_join(dbcon, chain_capture)\n\nPulling capture ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nSelect columns of interest, filter to date\n\n# capture table, select\ndb_capture &lt;- db_capture_sup %&gt;%\n  select(species_capture,\n         body_temp_c,\n         life_stage,\n         sex,\n         capture_animal_state,\n         bd_swab_id,\n         survey_id,\n         site,\n         date,\n         country_name) %&gt;%\n  filter(date &gt;= \"2015-01-01\")\n\n\n\nJoin with Bd table\n\ndb_bd_results &lt;- tbl(dbcon, Id(\"survey_data\", \"bd_qpcr_results\")) %&gt;%\n  select(bd_swab_id,\n         detected,\n         average_target_quant)\n\n# inner join to keep only captures with corresponding bd qpcr results\ndb_capture_bd &lt;- db_capture %&gt;%\n  inner_join(db_bd_results, by=\"bd_swab_id\")\n\n\n\nExplore # of samples by life stage, then filter\n\ndb_capture_bd %&gt;%\n  select(life_stage) %&gt;%\n  group_by(life_stage) %&gt;%\n  summarise(row_count = n()) %&gt;%\n  arrange(desc(row_count)) %&gt;%\n  collect()\n\n# A tibble: 13 × 2\n   life_stage              row_count\n   &lt;chr&gt;                     &lt;int64&gt;\n 1 adult                       22746\n 2 &lt;NA&gt;                         4515\n 3 juvenile                     3935\n 4 tadpole                      1666\n 5 subadult                      984\n 6 aquatic_larvae                573\n 7 metamorph                     428\n 8 larva                         417\n 9 unknown                       367\n10 terrestrial_development       330\n11 larvae                        218\n12 eggmass                         7\n13 Unknown                         2\n\ndb_capture_bd_life &lt;- db_capture_bd %&gt;%\n  filter(life_stage %in% c(\"juvenile\",\n                           \"subadult\",\n                           \"adult\"),\n         !is.na(life_stage))\n\n\n\nExplore # of samples by species, then filter\n\n(spp_summary &lt;- db_capture_bd_life%&gt;%\n  select(species_capture) %&gt;%\n  group_by(species_capture) %&gt;%\n  summarise(sample_count = n()) %&gt;%\n  arrange(desc(sample_count)) %&gt;%\n  collect())\n\n# A tibble: 131 × 2\n   species_capture           sample_count\n   &lt;chr&gt;                          &lt;int64&gt;\n 1 rana_muscosa                     13328\n 2 rana_clamitans                    2366\n 3 rana_catesbeiana                  1601\n 4 pseudacris_crucifer               1118\n 5 lithobates_sphenocephalus          965\n 6 rana_pipiens                       551\n 7 notophthalmus_viridescens          535\n 8 lithobates_chiricahuensis          453\n 9 colostethus_panamensis             429\n10 hyla_versicolor                    417\n# ℹ 121 more rows\n\n(spp_list &lt;- spp_summary %&gt;%\n  filter(sample_count &gt;= 100) %&gt;%\n  pull(species_capture))\n\n [1] \"rana_muscosa\"              \"rana_clamitans\"           \n [3] \"rana_catesbeiana\"          \"pseudacris_crucifer\"      \n [5] \"lithobates_sphenocephalus\" \"rana_pipiens\"             \n [7] \"notophthalmus_viridescens\" \"lithobates_chiricahuensis\"\n [9] \"colostethus_panamensis\"    \"hyla_versicolor\"          \n[11] \"hyla_chrysoscelis\"         \"anaxyrus_americanus\"      \n[13] \"silverstoneia_flotator\"    \"lithobates_warszewitschii\"\n[15] \"anaxyrus_fowleri\"          \"acris_blanchardi\"         \n[17] \"rhaebo_haematiticus\"       \"sachatamia_albomaculata\"  \n[19] \"ambystoma_opacum\"          \"hyla_cinerea\"             \n[21] \"lithobates_sylvaticus\"     \"pseudacris_feriarum\"      \n[23] \"smilisca_sila\"             \"plethodon_glutinosis\"     \n[25] \"ambystoma_maculatum\"       \"plethodon_cinereus\"       \n[27] \"unknown_species\"           \"desmognathus_fuscus\"      \n[29] \"lithobates_blairi\"        \n\ndb_capture_bd_life_spp &lt;- db_capture_bd_life %&gt;%\n  filter(species_capture %in% spp_list,\n         !is.na(species_capture))\n\n\n\ncollect data\n\ndata_capture_bd_query &lt;- db_capture_bd_life_spp %&gt;%\n  collect()\n\ncolnames(data_capture_bd_query)\n\n [1] \"species_capture\"      \"body_temp_c\"          \"life_stage\"          \n [4] \"sex\"                  \"capture_animal_state\" \"bd_swab_id\"          \n [7] \"survey_id\"            \"site\"                 \"date\"                \n[10] \"country_name\"         \"detected\"             \"average_target_quant\"\n\nhead(data_capture_bd_query)\n\n# A tibble: 6 × 12\n  species_capture   body_temp_c life_stage sex   capture_animal_state bd_swab_id\n  &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;     \n1 rhaebo_haematiti…        NA   juvenile   unkn… alive                150608_04 \n2 rhaebo_haematiti…        NA   juvenile   unkn… alive                150608_05 \n3 colostethus_pana…        NA   adult      unkn… alive                150610_01 \n4 rhaebo_haematiti…        26.7 adult      unkn… alive                150612_01 \n5 rhaebo_haematiti…        25.7 juvenile   unkn… alive                150612_02 \n6 rhaebo_haematiti…        26.6 adult      unkn… alive                150612_03 \n# ℹ 6 more variables: survey_id &lt;chr&gt;, site &lt;chr&gt;, date &lt;date&gt;,\n#   country_name &lt;chr&gt;, detected &lt;dbl&gt;, average_target_quant &lt;dbl&gt;\n\n\nThese data are ready to be analyzed and visualized!"
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#disconnect",
    "href": "tutorial_series/05_data_workflow.html#disconnect",
    "title": "4. Data Workflow",
    "section": "Disconnect",
    "text": "Disconnect\n\ndbDisconnect(dbcon)"
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#setup-1",
    "href": "tutorial_series/05_data_workflow.html#setup-1",
    "title": "4. Data Workflow",
    "section": "Setup",
    "text": "Setup\n\n# minimal packages for RIBBiTR DB Workflow\nimport ibis\nfrom ibis import _\nimport pandas as pd\nimport dbconfig\nimport db_access as db\n\n# establish database connection\ndbcon = ibis.postgres.connect(**dbconfig.ribbitr)\n\n# load table metadata\nmdt = dbcon.table(database = \"public\", name = \"all_tables\").to_pandas()\n\n# load column metadata\nmdc = (\n  dbcon.table(database=\"public\", name=\"all_columns\")\n  .filter(_.table_schema == 'survey_data')\n  .to_pandas()\n  )"
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#data-discovery-and-pulling-1",
    "href": "tutorial_series/05_data_workflow.html#data-discovery-and-pulling-1",
    "title": "4. Data Workflow",
    "section": "Data discovery and pulling",
    "text": "Data discovery and pulling\nSuppose we are interested in capture and Bd swab data. Specifically, we want to compare Bd qPCR results for juvenile-to-adult individuals, captured in 2015 or later, across species and sites.\nLooking at the table metadata, the two observation tables with most of the data of interest are called “capture” and “bd_qpcr_results”.\n\nJoin with support tables, filter to date\n\n# Ponters for all tables\ndb_capture = dbcon.table('capture', database='survey_data')\ndb_survey = dbcon.table('survey', database='survey_data')\ndb_visit = dbcon.table('visit', database='survey_data')\ndb_site = dbcon.table('site', database='survey_data')\ndb_region = dbcon.table('region', database='survey_data')\ndb_country = dbcon.table('country', database='survey_data')\n\n# Recursive joins\ndb_capture_sup = (\n    db_capture\n    .join(db_survey, db_capture.survey_id == db_survey.survey_id)\n    .join(db_visit, db_survey.visit_id == db_visit.visit_id)\n    .join(db_site, db_visit.site_id == db_site.site_id)\n    .join(db_region, db_site.region_id == db_region.region_id)\n    .join(db_country, db_region.country_id == db_country.country_id)\n)\n\nOr alternatively:\n\n# create chain object\nchain_capture = db.tbl_chain(\"capture\", mdc)\n\n# join recursively, filter by date\ndb_capture_sub = db.tbl_join(dbcon, chain_capture, tbl=db_capture, join=\"left\")\n\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n\n\n\nSelect columns of interest, filter to date\n\n# capture table, select, filter\ndb_capture = (\n  db_capture_sub\n  .select([\n    'species_capture',\n    'body_temp_c',\n    'life_stage',\n    'sex',\n    'capture_animal_state',\n    'bd_swab_id',\n    'survey_id',\n    'site',\n    'date',\n    'country_name',\n    ])\n    .filter(_.date &gt;= '2015-01-01')\n  )\n\n\n\nJoin with Bd table\n\n# bd qpcr results lazy table\ndb_bd_results = (\n  dbcon.table(database=\"survey_data\", name=\"bd_qpcr_results\")\n  .select([\n    'bd_swab_id',\n    'detected',\n    'average_target_quant'\n  ])\n  )\n\n# inner join to keep only captures with corresponding bd qpcr results\ndb_capture_bd = (\n  db_capture\n  .inner_join(db_bd_results, db_capture.bd_swab_id == db_bd_results.bd_swab_id)\n  )\n\n\n\nExplore # of samples by life stage, then filter\n\n# count by life stage\nlife_stage_counts = (\n  db_capture_bd\n  .group_by('life_stage')\n  .aggregate(row_count=_.count())\n  .order_by(_.row_count.desc())\n  .to_pandas()\n  )\n\nprint(life_stage_counts)\n\n                 life_stage  row_count\n0                     adult      22746\n1                      None       4515\n2                  juvenile       3935\n3                   tadpole       1666\n4                  subadult        984\n5            aquatic_larvae        573\n6                 metamorph        428\n7                     larva        417\n8                   unknown        367\n9   terrestrial_development        330\n10                   larvae        218\n11                  eggmass          7\n12                  Unknown          2\n\n\n# filter to desired life stages\ndb_capture_bd_life = (\n  db_capture_bd\n  .filter(_.life_stage.isin(['juvenile','subadult', 'adult']) & _.life_stage.notnull())\n  )\n\n\n\nExplore # of samples by species, then filter\n\n# count by species\nspp_summary = (\n  db_capture_bd_life\n  .group_by('species_capture')\n  .aggregate(sample_count=_.count())\n  .order_by(_.sample_count.desc())\n  .to_pandas()\n  )\nprint(spp_summary)\n\n                  species_capture  sample_count\n0                    rana_muscosa         13328\n1                  rana_clamitans          2366\n2                rana_catesbeiana          1601\n3             pseudacris_crucifer          1118\n4       lithobates_sphenocephalus           965\n..                            ...           ...\n126             silverstoneia_spp             1\n127                 diasporus_spp             1\n128        craugastor_monnichorum             1\n129               acris_crepitans             1\n130  hyalinobatrachium_talamancae             1\n\n[131 rows x 2 columns]\n\n\n# generate species list\nspp_list = spp_summary[spp_summary['sample_count'] &gt;= 100]['species_capture'].tolist()\n\n# filter to species in spp_list\ndb_capture_bd_life_spp = (\n  db_capture_bd_life\n  .filter(_.species_capture.isin(spp_list) & _.species_capture.notnull())\n  )\n\n\n\nPull data\n\n# pull data\ndata_capture_bd_query = db_capture_bd_life_spp.to_pandas()\n\n# printcolumn names\ndata_capture_bd_query.columns\n\nIndex(['species_capture', 'body_temp_c', 'life_stage', 'sex',\n       'capture_animal_state', 'bd_swab_id', 'survey_id', 'site', 'date',\n       'country_name', 'detected', 'average_target_quant'],\n      dtype='object')\n\n# preview data\ndata_capture_bd_query.head()\n\n             species_capture  body_temp_c  ... detected average_target_quant\n0        rhaebo_haematiticus          NaN  ...      0.0                 0.00\n1  lithobates_warszewitschii         18.6  ...      1.0              1007.69\n2  lithobates_warszewitschii          NaN  ...      1.0               257.43\n3  lithobates_warszewitschii         21.8  ...      1.0                56.15\n4  lithobates_warszewitschii         21.6  ...      0.0                 0.00\n\n[5 rows x 12 columns]\n\n\nThese data are ready to be analyzed and visualized!"
  },
  {
    "objectID": "tutorial_series/05_data_workflow.html#disconnect-1",
    "href": "tutorial_series/05_data_workflow.html#disconnect-1",
    "title": "4. Data Workflow",
    "section": "Disconnect",
    "text": "Disconnect\n\n# close connection\ndbcon.disconnect()"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#recursive-joins",
    "href": "tutorial_series/04_table_joins.html#recursive-joins",
    "title": "3. Data Pulling",
    "section": "Recursive joins",
    "text": "Recursive joins\nWith that understanding, let’s connect the data we have been wanting all along:\n\n# pointers for all tables\ndb_ves &lt;- tbl(dbcon, Id(\"survey_data\", \"ves\"))\ndb_survey &lt;- tbl(dbcon, Id(\"survey_data\", \"survey\"))\ndb_visit &lt;- tbl(dbcon, Id(\"survey_data\", \"visit\"))\ndb_site &lt;- tbl(dbcon, Id(\"survey_data\", \"site\"))\ndb_region &lt;- tbl(dbcon, Id(\"survey_data\", \"region\"))\ndb_country &lt;- tbl(dbcon, Id(\"survey_data\", \"country\"))\n\ndb_ves_sup &lt;- db_ves %&gt;%\n  left_join(db_survey, by = \"survey_id\") %&gt;%\n  left_join(db_visit, by = \"visit_id\") %&gt;%\n  left_join(db_site, by = \"site_id\") %&gt;%\n  left_join(db_region, by = \"region_id\") %&gt;%\n  left_join(db_country, by = \"country_id\")\n\nFinally we can select for and filter the variables of interest:\n\ndata_ves_filtered &lt;- db_ves_sup %&gt;%\n  select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id,\n         site,\n         date,\n         country_name) %&gt;%\n  filter(life_stage == \"adult\",\n         year(date) == \"2022\",\n         country_name == \"brazil\") %&gt;%\n  collect()"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#links-chains-recursive-joins",
    "href": "tutorial_series/04_table_joins.html#links-chains-recursive-joins",
    "title": "3. Data Pulling",
    "section": "Links, Chains, Recursive Joins",
    "text": "Links, Chains, Recursive Joins\nWe developed some additional functions in the ribbitrrr package to help us avoid the tedium of consulting the database schema diagram or column metadata. The workflow for linking tables one at a time works like this:\n\n# create a link object for table ves: \"which tables are 1 step away\"\nlink_ves &lt;- tbl_link(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey &lt;- tbl_left_join(dbcon, link_ves)\n\nPulling ves ... done.\nJoining with survey ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"weather_condition_notes\"        \"pressure_psi_drop\"             \n[47] \"relative_humidity_percent\"      \"relative_humidity_drop_percent\"\n[49] \"wind_speed_min_m_s\"             \"wind_speed_max_m_s\"            \n[51] \"air_temp_c_drop\"                \"densiometer_d1_num_covered\"    \n[53] \"d1_n\"                           \"d1_s\"                          \n[55] \"d1_e\"                           \"d1_w\"                          \n[57] \"d1_percent_cover\"               \"densiometer_d2_num_covered\"    \n[59] \"d2_n\"                           \"d2_s\"                          \n[61] \"d2_e\"                           \"d2_w\"                          \n[63] \"d2_percent_cover\"               \"depth_of_water_from_d2_cm\"     \n[65] \"vegetation_cover_percent\"       \"vegetation_notes\"              \n[67] \"secchi_depth_cm\"                \"visit_id\"                      \n\n\nGreat, similar results to our previous manual join, but do I need to do this recursively to get to the country table? Is there another way?\nThe workflow for linking tables recursively works like this:\n\n# create a chain (or recusive link) object for table ves: \"which tables are any number of steps away\"\nchain_ves &lt;- tbl_chain(\"ves\", mdc)\n\n# join tables in link object\ndb_ves_survey &lt;- tbl_left_join(dbcon, chain_ves)\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# check columns\ncolnames(db_ves_survey)\n\n [1] \"species_ves\"                    \"count_ves\"                     \n [3] \"detection_location\"             \"microhab\"                      \n [5] \"life_stage\"                     \"sex\"                           \n [7] \"comments_ves\"                   \"microhab_moredetail\"           \n [9] \"observer_ves\"                   \"visual_animal_state\"           \n[11] \"ves_id\"                         \"survey_id\"                     \n[13] \"start_time\"                     \"end_time\"                      \n[15] \"detection_type\"                 \"duration_minutes\"              \n[17] \"observers_survey\"               \"wind_speed_m_s\"                \n[19] \"air_temp_c\"                     \"water_temp_c\"                  \n[21] \"p_h\"                            \"tds_ppm\"                       \n[23] \"comments_survey\"                \"wind\"                          \n[25] \"sky\"                            \"air_time\"                      \n[27] \"water_time\"                     \"fish\"                          \n[29] \"description\"                    \"survey_quality\"                \n[31] \"transect\"                       \"number_observers\"              \n[33] \"samp_loc\"                       \"pressure_psi\"                  \n[35] \"relative_humidty_percent\"       \"dissolved_o2_percent\"          \n[37] \"salinity_ppt\"                   \"cloud_cover_percent\"           \n[39] \"precip\"                         \"soil_humidity_m3m3\"            \n[41] \"wind_speed_scale\"               \"precipitation_during_visit\"    \n[43] \"precipitation_last_48_h\"        \"temperature_last_48_h\"         \n[45] \"weather_condition_notes\"        \"pressure_psi_drop\"             \n[47] \"relative_humidity_percent\"      \"relative_humidity_drop_percent\"\n[49] \"wind_speed_min_m_s\"             \"wind_speed_max_m_s\"            \n[51] \"air_temp_c_drop\"                \"densiometer_d1_num_covered\"    \n[53] \"d1_n\"                           \"d1_s\"                          \n[55] \"d1_e\"                           \"d1_w\"                          \n[57] \"d1_percent_cover\"               \"densiometer_d2_num_covered\"    \n[59] \"d2_n\"                           \"d2_s\"                          \n[61] \"d2_e\"                           \"d2_w\"                          \n[63] \"d2_percent_cover\"               \"depth_of_water_from_d2_cm\"     \n[65] \"vegetation_cover_percent\"       \"vegetation_notes\"              \n[67] \"secchi_depth_cm\"                \"visit_id\"                      \n[69] \"date\"                           \"survey_time\"                   \n[71] \"campaign\"                       \"visit_status\"                  \n[73] \"comments_visit\"                 \"site_id\"                       \n[75] \"site\"                           \"utm_zone\"                      \n[77] \"utme\"                           \"utmn\"                          \n[79] \"area_sqr_m\"                     \"site_code\"                     \n[81] \"elevation_m\"                    \"depth_m\"                       \n[83] \"topo\"                           \"wilderness\"                    \n[85] \"site_comments\"                  \"region_id\"                     \n[87] \"region\"                         \"country_id\"                    \n[89] \"location_id\"                    \"country_name\"                  \n[91] \"iso_country_code\"              \n\n\nHooray! Also yikes, that’s a lot of columns! I am starting to see why we store all these in separate tables!\nLet’s use the chain workflow to join only the data we want, to filter to Brazil data.\n\n# create chain object\nchain_ves &lt;- tbl_chain(\"ves\", mdc)\n\n# join recursively, specifying desired columns, filter, collect\ndb_ves_adult_final &lt;- tbl_left_join(dbcon, chain_ves) %&gt;%\n    select(species_ves,\n         count_ves,\n         life_stage,\n         sex,\n         survey_id,\n         site,\n         date,\n         country_name) %&gt;%\n  filter(life_stage == \"adult\",\n         year(date) == \"2022\",\n         country_name == \"brazil\")\n\nPulling ves ... done.\nJoining with survey ... done.\nJoining with visit ... done.\nJoining with site ... done.\nJoining with region ... done.\nJoining with country ... done.\n\n# pull selected, filtered, joined data\ndata_ves_adult_final &lt;- db_ves_adult_final %&gt;%\n  collect()\n\nNote that there are corresponding recursive join functions in ribbitrrr for:\n\ntbl(_left_join()\ntbl(_inner_join()\ntbl(_full_join()\ntbl(_right_join()"
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#the-need-for-joins-1",
    "href": "tutorial_series/04_table_joins.html#the-need-for-joins-1",
    "title": "3. Data Pulling",
    "section": "The need for joins",
    "text": "The need for joins\nLet’s begin with the query we constructed in our last tutorial:\n\n# lazy table select, filter, and collect all in one\nves_adult = (\n  dbcon.table(database=\"survey_data\", name=\"ves\")\n  .select([\n    'species_ves',\n    'count_ves',\n    'life_stage',\n    'sex',\n    'survey_id'\n  ])\n  .filter(_.life_stage == 'adult')\n  .to_pandas()\n  )\n\n“This is all good and well, but I only want data from Brazil… and there is no country information in this table! How do I connect with and filter by country? or what if I want data from 2022 only?”\nSo far we have practiced pulling data from individual tables, but often we find that the data we need is stored accross multiple tables? We have to “join” these tables to bring the data we need together."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#table-join-basics-1",
    "href": "tutorial_series/04_table_joins.html#table-join-basics-1",
    "title": "3. Data Pulling",
    "section": "Table join basics",
    "text": "Table join basics\nRecall that our database is not just a bunch of tables, it is a bunch of tables with relationships. For example, we can see that our VES table (db_ves_adult) has a column named survey_id. Taking a closer look at the column metadata (mdc), the survey table also has a column named survey_id. This common column is key to connecting our data between tables.\n\nUnderstanding Keys\nThe concept of key columns or “keys” in database tables is used to help organize and communicate the relationships we want to establish between tables. There are several types of keys, here we will introduce 3:\n\nPrimary Key (pk or pkey) – a column which is a unique identifier for each record (row) in a database table, ensuring that each record can be uniquely distinguished from all others in the table. Ideally a single column, often an ID\nForeign Key (fk or fkey) – a column in one table which refers to the primary key in another table, establishing an asymmetric relationship between the two tables.\nNatural Key (nk or nkey) – a meaningful column (or set of columns) in a table which “naturally” and uniquely constrains all other columns. Often multiple columns, used to collectively define an ID column to be used as a primary key. Maintained in metadata as important columns, not always tracked in database relationships.\n\nWhen we pull a data table from a database, it is often not so obvious which columns are or could be key columns, and which type of key. Luckily, we have column metadata to help us keep track of this! Check out the primary_key and foreign_key columns for the survey table:\n\nves_metadata = mdc[(mdc['table_name'] == 'ves')][['table_name', 'column_name', 'primary_key', 'foreign_key', 'natural_key']]\n\nprint(ves_metadata)\n\n    table_name          column_name primary_key foreign_key  natural_key\n93         ves               ves_id        True       False         True\n119        ves  visual_animal_state       False       False        False\n314        ves         comments_ves       False       False        False\n319        ves            count_ves       False       False        False\n320        ves   detection_location       False       False        False\n321        ves           life_stage       False       False        False\n322        ves             microhab       False       False        False\n323        ves  microhab_moredetail       False       False        False\n324        ves         observer_ves       False       False        False\n325        ves                  sex       False       False        False\n326        ves          species_ves       False       False        False\n327        ves            survey_id       False        True        False\n\n\nWe can see here that ves_id is the primary key (ie. unique, non-null row identifier) for the ves table (ves_id is also the only natural key for this table). We also see that column survey_id is a foreign key, meaning it points to the primary key of another table. Good investigation work, but this is tedious. Is there another way?\n\n## data_access key functions\n\n# primary key for ves table\ndb.tbl_pkey(\"ves\", mdc)\n\n['ves_id']\n\n# foreign key for ves table\ndb.tbl_fkey(\"ves\", mdc)\n\n['survey_id']\n\n# natural key for ves table\ndb.tbl_nkey(\"ves\", mdc)\n\n['ves_id']\n\n# all unique key columns for ves table\ndb.tbl_keys(\"ves\", mdc)\n\n['survey_id', 'ves_id']\n\n\nNotice that we passed the column metadata to these functions, to help us automate key lookups."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#joining-tables-by-keys-1",
    "href": "tutorial_series/04_table_joins.html#joining-tables-by-keys-1",
    "title": "3. Data Pulling",
    "section": "Joining tables by keys",
    "text": "Joining tables by keys\nWe can use these key columns, and what we know about the structure of our database, to join related tables. For example, let’s join the ves and survey tables along the ves foreign key of survey_id:\n\n# ves lazy table\ndb_ves = dbcon.table(database=\"survey_data\", name=\"ves\")\n# survey lazy table\ndb_survey = dbcon.table(database=\"survey_data\", name=\"survey\")\n\n# lef_join\nleft_ves_survey = db_ves.left_join(db_survey, \"survey_id\")\n\n# check columns\nleft_ves_survey.columns\n\n['species_ves', 'count_ves', 'detection_location', 'microhab', 'life_stage', 'sex', 'comments_ves', 'microhab_moredetail', 'observer_ves', 'visual_animal_state', 'ves_id', 'survey_id', 'start_time', 'end_time', 'detection_type', 'duration_minutes', 'observers_survey', 'wind_speed_m_s', 'air_temp_c', 'water_temp_c', 'p_h', 'tds_ppm', 'comments_survey', 'wind', 'sky', 'air_time', 'water_time', 'fish', 'description', 'survey_quality', 'transect', 'number_observers', 'samp_loc', 'pressure_psi', 'relative_humidty_percent', 'dissolved_o2_percent', 'salinity_ppt', 'cloud_cover_percent', 'precip', 'soil_humidity_m3m3', 'wind_speed_scale', 'precipitation_during_visit', 'precipitation_last_48_h', 'temperature_last_48_h', 'weather_condition_notes', 'pressure_psi_drop', 'relative_humidity_percent', 'relative_humidity_drop_percent', 'wind_speed_min_m_s', 'wind_speed_max_m_s', 'air_temp_c_drop', 'densiometer_d1_num_covered', 'd1_n', 'd1_s', 'd1_e', 'd1_w', 'd1_percent_cover', 'densiometer_d2_num_covered', 'd2_n', 'd2_s', 'd2_e', 'd2_w', 'd2_percent_cover', 'depth_of_water_from_d2_cm', 'vegetation_cover_percent', 'vegetation_notes', 'secchi_depth_cm', 'survey_id_right', 'visit_id']\n\n\nWe see that the columns of db_ves_survey correspond to the union of those from both the ves and survey tables. More importantly, the rows are lined up using survey_id as the key “join by” column. You could also substitute this with by = tbl_fkey(\"ves\", mdc).\nWhen we join two tables along a key column, we are asking to align rows in the two tables where this key column is equal. In this example, we are returning a wider table where rows from the VES table are aligned with rows from the survey table where survey_id is equal in both.\nLet’s take a closer look at the number of rows in our tables to understand better what is going on.\n\n# Count rows in db_ves\nprint(db_ves.count().execute())\n\n28390\n\n# Count rows in db_survey\nprint(db_survey.count().execute())\n\n23111\n\n# Count rows in left_ves_survey\nprint(left_ves_survey.count().execute())\n\n28390\n\n\nNotice that in this case, the number of rows in the joined db_ves_survey table is equal to the number ofrows in the db_ves table. This has to do with the type of join we used to join these tables, in this case a “left join”.\n\nTypes of joins\nJoins unite rows from two tables along shared key column values, allowing unshared columns between the two tables to be brought into a single table (as we see above in db_ves_survey). However,here are different joins to handle key columns vales that are not shared between tables (ie. are unique to one table). The following graphic helps illustrate the different join types:\n\n\n\nTypes of dplyr joins. Graphic by Software Carpentry used under Creative Commons 4.0\n\n\nThe graphic above describes the treatment of key column values which are either unique to one table or shared in both, using Venn diagram. In our db_ves_survey example, our use of left_join() essentially says “return everything found in the 1st (”left”) table (ie. db_ves) regardless of if it has a match in the 2nd (“right”) table (ie. db_survey). This explains why the output row numbers are identical to those in db_ves.\nUsing this diagram, let’s try a different scenario. Let’s instead use a full_join() to return all rows from each table, regardless of whether they have a matching survey_id in the other.\n\n# Full join of db_ves and db_survey\nfull_ves_survey = db_ves.join(db_survey, db_ves.survey_id == db_survey.survey_id, how='outer')\n\n# count rows\nprint(full_ves_survey.count().execute())\n\n41689\n\n\nIn this case we see a greater number of rows returned than are present in either table (though less than the sum of rows in each table). This tells us that some rows did align between tables, while others did not. Specifically, full_ves_survey_rows returns all VES observations (which have a corresponding survey_id in the survey table by design, see note below) as well as all other surveys in the survey table, even those that do not correspond to a VES survey. We can see this quickly by comparing the distinct detection_type’s seen in each joined table:\n\n# Distinct detection_types in left_ves_survey\nprint(left_ves_survey.select(['detection_type']).distinct().execute())\n\n  detection_type\n0         visual\n\n# Distinct detection_types in full_ves_survey\nprint(full_ves_survey.select(['detection_type']).distinct().execute())\n\n  detection_type\n0          aural\n1          other\n2         visual\n3        capture\n\n\nNote: In this case, the survey table is intentionally designed such that all survey_id’s in the ves table. To confirm this we can try:\n\n# Anti-join of db_ves and db_survey\nanti_ves_survey = db_ves.anti_join(db_survey, db_ves.survey_id == db_survey.survey_id)\nanti_ves_survey_rows = anti_ves_survey.count().execute()\nprint(anti_ves_survey_rows)\n\n0\n\n\nThere are 0 rows in this anti_join. This means that in our Venn diagram, the left lobe is essentially empty or nonexistent. The outcome is that in this case, left_join() and inner_join() are equivalent, as are full_join() and right_join(). This is not true in general, however.\n\nRelationship cardinality\nForeign keys which point to primary keys are an example of a many:1 (“many-to-one”) relationship This is to say that a foreign key column can hold any number of duplicate values, while a primary key value can show up a maximum of once. Many:1 relationships are most common in our database. Occasionally you may see a 1:1 relationship or a many:many relationship.\n\n\n\nDeciding how to join tables\nYou have the liberty to decide which join you want, so how do you make that choice? Do you want - all the data from one table (left_join() or right_join()) - all the data or from both tables (full_join) - only data that is shared between the two (inner_join())\nTo join tables properly along shared and key columns, it is important to understand the structure of the database. This is where consulting the column metadata (mdc) or schema diagram comes in handy.\nLooking at the schema diagram, we see that to join the VES data with country, we will have to recursively join the survey, visit, site, region, and country tables."
  },
  {
    "objectID": "tutorial_series/04_table_joins.html#recursive-joins-1",
    "href": "tutorial_series/04_table_joins.html#recursive-joins-1",
    "title": "3. Data Pulling",
    "section": "Recursive joins",
    "text": "Recursive joins\nWith that understanding, let’s connect the data we have been wanting all along:\n\n# Ponters for all tables\ndb_ves = dbcon.table('ves', database='survey_data')\ndb_survey = dbcon.table('survey', database='survey_data')\ndb_visit = dbcon.table('visit', database='survey_data')\ndb_site = dbcon.table('site', database='survey_data')\ndb_region = dbcon.table('region', database='survey_data')\ndb_country = dbcon.table('country', database='survey_data')\n\n# Recursive joins\ndb_ves_sup = (\n    db_ves\n    .join(db_survey, db_ves.survey_id == db_survey.survey_id)\n    .join(db_visit, db_survey.visit_id == db_visit.visit_id)\n    .join(db_site, db_visit.site_id == db_site.site_id)\n    .join(db_region, db_site.region_id == db_region.region_id)\n    .join(db_country, db_region.country_id == db_country.country_id)\n)\n\nFinally we can select for and filter the variables of interest:\n\ndata_ves_filtered = (\n    db_ves_sup\n    .select(\n      'species_ves',\n      'count_ves',\n      'life_stage',\n      'sex',\n      'survey_id',\n      'site',\n      'date',\n      'country_name')\n    .filter([\n        (_.life_stage == 'adult') &\n        (_.date.year() == 2022) &\n        (_.country_name == 'brazil')\n    ])\n    .to_pandas()\n)"
  }
]